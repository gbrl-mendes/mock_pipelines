---
title: "Análise MTG05 Setembro/2024"
author: "OH, Heron; Mendes, Gabriel A."
date: '03/09/2024'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Meta-Pipeline

This pipeline integrates public tools available for metabarcoding analyses. To share or reproduce this content, please require authors' consent.\
Contact**:** [lgc.edna\@gmail.com](mailto:lgc.edna@gmail.com){.email} or [heronoh\@gmail.com](mailto:heronoh@gmail.com){.email}

## Short introduction

Welcome! We will guide you trought the analysis of environmental eDNA metabarcoding.

To proceed you will need the raw reads files and a .csv file with:

a)  a column listing all samples, with unique names identical to raw reads radicals (*File_name*).

b)  a column with the respective primer used for each sample.

c)  other columns for any metadata available.

## Preparing analyses working directory

```{bash}

# Creating a folder for all analyses files

## Saving project path into bash variable
PRJCT_DIR=/home/gabriel/projetos/mock_pipelines;

## Checking variable
echo $PRJCT_DIR; 

## Creating folder
mkdir -p $PRJCT_DIR;

## Creating data folder
mkdir -p $PRJCT_DIR/data
```

## Getting raw reads

```{bash}

# 1 - Creating run files folder

## Creating run files folder
mkdir -p $PRJCT_DIR/raw_reads 

## Navigate to run files folder
cd $PRJCT_DIR/raw_reads 

## Store raw data path into bash variable
RAW_DATA=/home/gabriel/projetos/mock_pipelines/raw_reads

# 2a - Or downloading from Illumina BaseSpace

## Authenticating to BaseSpace
bs auth;

## List projects data
bs list datasets;

## Downloading demultiplexed read files
bs download project -n "XXXXXXXX" -o fastq --extension=fastq.gz

# 2b - Or copying to run files folder

cp /home/teixeiradf/gabriel/*.fastq $RAW_DATA;

```

## Raw reads quality check

```{bash, eval=FALSE}

# 1 - Creat folders for quality checking files

mkdir $PRJCT_DIR/quality;

mkdir $PRJCT_DIR/quality/raw_reads/

mkdir $PRJCT_DIR/quality/raw_reads/multiqc

# 2 - Run FastQC on all reads 

ls $RAW_DATA/*

## Running one at a time (1st possibility)
fastqc $RAW_DATA/* --outdir $PRJCT_DIR/quality/raw_reads/

## Run in parallel (2nd possibility)
find $RAW_DATA/raw -name '*.fastq.gz' 2>/dev/null | parallel fastqc {1} -o $PRJCT_DIR/quality/
find $RAW_DATA/raw/* -name '*.fastq' 2>/dev/null | parallel fastqc {1} -o $PRJCT_DIR/quality/

# 3 - Run MULTIqc on FastQC files to integrate results

multiqc --interactive $PRJCT_DIR/quality --outdir $PRJCT_DIR/quality/raw_reads/multiqc 

# 4 - Now navigate on the files Rstudio pannel to open MULTIqc report on web browser and view results
```

## Demultiplexing with Cutadapt

For more information, access: <https://cutadapt.readthedocs.io/en/stable/guide.html#combinatorial-demultiplexing>

```{bash, eval=FALSE}

# 1 - Creating demuxed files folders

mkdir $PRJCT_DIR/dmux/

RAW_DATA=/home/gabriel/projetos/analises/metagen/MTG05_0924/dmux/raw;

mkdir $RAW_DATA;

mkdir $PRJCT_DIR/dmux/raw_combined

# 2- Creating fasta file with index-primer sequences

barcodes_tbl <- 
  readr::read_csv(file = "~/projetos/analises/metagen/MTG05_0924/data/MTG05_0924-primers_n_samples.csv") %>% 
  dplyr::select(tidytable::starts_with(c("Index"))) %>% 
  unite(ends_with("FWD"), col = "FWD",sep = "--",remove = T) %>% 
  unite(ends_with("REV"), col = "REV",sep = "--",remove = T) %>% 
  pivot_longer(
    cols = c("FWD", "REV"),
    values_to = "Index") %>% 
  # dplyr::select(-c("name")) %>% 
  mutate("Index" = paste0(">",Index)) %>% 
  unique() %>% 
  tidyr::separate(col = "Index",
                  sep = "--",
                  into = c("Index name", "Index sequence")) %>% 
  dplyr::arrange(`Index name`)
  
View(barcodes_tbl)

## Write fasta file with ASVs and Taxonomy
  barcodes_fasta <- c(rbind(barcodes_tbl$`Index name`, barcodes_tbl$`Index sequence`))

write(x = barcodes_fasta,
      file = "/home/gabriel/projetos/analises/metagen/MTG05_0924/data/barcodes.fasta")

# 3 - Combine files to demultiplex at once

## Nao usado, ja que tenho apenas dois arquivos nesta analise.
## Usar o seguinte codigo apenas se tiver diferentes R1 e R2

# cat $PRJCT_DIR/raw_reads/*R1* > $PRJCT_DIR/raw_reads/run1_ITS_COI_R1.fastq
# 
# cat $PRJCT_DIR/raw_reads/*R2* > $PRJCT_DIR/raw_reads/run1_ITS_COI_R2.fastq

# 3 - Defining server capacity

sysctl -w fs.file-max=100000

ulimit -n 1000000

## Reference: 
# https://www.cyberciti.biz/faq/linux-increase-the-maximum-number-of-open-files/

# 4 - Demultiplexing with Cutadapt

## Atencao!! Verificar o codigo abaixo para colocar o nome correto dos arquivos
## atuais fastq.gz que seram empregados na analise!

cutadapt -e 1 -j 72 --no-indels --max-n 0 --discard-untrimmed --action retain --match-read-wildcards --buffer-size=16000000 -g  file:/home/gabriel/projetos/analises/metagen/MTG05_0924/data/barcodes.fasta -G file:/home/gabriel/projetos/analises/metagen/MTG05_0924/data/barcodes.fasta -o /home/gabriel/projetos/analises/metagen/MTG05_0924/dmux/raw/{name1}-{name2}.R1.fastq -p /home/gabriel/projetos/analises/metagen/MTG05_0924/dmux/raw/{name1}-{name2}.R2.fastq /home/gabriel/projetos/analises/metagen/MTG05_0924/raw_reads/IPE_S1_L001_R1_001.fastq.gz /home/gabriel/projetos/analises/metagen/MTG05_0924/raw_reads/IPE_S1_L001_R2_001.fastq.gz
 
# 5 - Apagando arquivos vazios
find /home/gabriel/projetos/analises/metagen/MTG05_0924/dmux/raw/ -type f -empty -print -delete
```

## Combine demultiplexed FWD and REV files into single R1 & R2 pair

```{r, eval=FALSE,echo=FALSE, results='hide'}

# 1 - List dmux files 
dmux_files <- list.files("/home/gabriel/projetos/analises/metagen/MTG05_0924/dmux/raw",full.names = T)  

# 2 - Obtaining dmux files names 
names(dmux_files) <- dmux_files %>%
  str_replace(pattern = "^.*/",replacement = "") %>%
  str_replace(pattern = "fas.*.$",replacement = "")  

# 3 - Defining combined files folder 
dmux_combined_folder <- "/home/gabriel/projetos/analises/metagen/MTG05_0924/dmux/raw_combined"  
dir.create(path = dmux_combined_folder) 

# 4 - Loading primers_n_samples table  
primers_n_samples <- readr::read_csv(file = "/home/gabriel/projetos/analises/metagen/MTG05_0924/data/MTG05_0924-primers_n_samples.csv")  

View(primers_n_samples)  

# 4 - Combining the dmux files into single R1 & R2 pair  

## Defining files 

## Atencao!! Ter certeza que os nomes salvos nas colunas FWD rad R1, FWD rad R2,
## REV rad R1 e REV rad R2 correspondem aos arquivos que foram demultiplexados e
## estao contidos no arquivos dmux_files!!

  dmux_idxs <- readr::read_csv(file = "~/projetos/analises/metagen/MTG05_0924/data/MTG05_0924-primers_n_samples.csv") %>%   
  dplyr::select(c(tidyr::starts_with("Index name"),"File_name")) %>%   
  dplyr::mutate("FWD rad R1" = paste(.$`Index name FWD`, "-", .$`Index name REV`, ".R1.",sep = ""), 
                "FWD rad R2" = paste(.$`Index name FWD`, "-", .$`Index name REV`, ".R2.",sep = ""),
                "REV rad R1" = paste(.$`Index name REV`, "-", .$`Index name FWD`, ".R1.",sep = ""),
                "REV rad R2" = paste(.$`Index name REV`, "-", .$`Index name FWD`, ".R2.",sep = "")) %>%
  dplyr::group_by(File_name) %>%
  dplyr::mutate("FWD file R1" = dmux_files[grepl(`FWD rad R1`,x = dmux_files)],
                "FWD file R2" = dmux_files[grepl(`FWD rad R2`,x = dmux_files)],
                "REV file R1" = dmux_files[grepl(`REV rad R1`,x = dmux_files)],
                "REV file R2" = dmux_files[grepl(`REV rad R2`,x = dmux_files)]) %>%   
  dplyr::mutate("Combined R1" = paste0(dmux_combined_folder,"/",File_name,"-R1--raw.fastq.gz"),
                "Combined R2" = paste0(dmux_combined_folder,"/",File_name,"-R2--raw.fastq.gz")) %>%
  dplyr::ungroup()  

View(dmux_idxs)

## A uniao dos arquivos FWD e REV em arquivos R1 e R2 retira o nome dos index ## dos arquivos, mantendo apenas o amostra.R1.fastq e amostra.R2.fastq  

## Combining 
for (sample in 1:nrow(dmux_idxs)) {      
  print(paste0("Working on sample ",dmux_idxs$File_name[sample] ))   
  Rfastp::catfastq(output = dmux_idxs$`Combined R1`[sample],
                   inputFiles = c(dmux_idxs$`FWD file R1`[sample], dmux_idxs$`REV file R2`[sample]))      
  Rfastp::catfastq(output = dmux_idxs$`Combined R2`[sample],
                   inputFiles = c(dmux_idxs$`FWD file R2`[sample], dmux_idxs$`REV file R1`[sample])) }  

## Now the 4 files are combined into a single pair with names! ## Can be run into non-dmx pipe normally!
```

## Demux reads quality checking with FastQC and MultiQC

É profundamente necessario realizar a avaliacao da qualidade das reads neste momento. Se tiver um numero muito baixo de reads por amostra, significa que o Cutadapt nao encontrou os primers/index nos arquivos e alguma coisa deve ser feita.

```{bash}

# 1 - Creat folder for quality checking files

mkdir $PRJCT_DIR/quality/dmux;

# 2 - Run FastQC on all reads 

## Running one at a time (1st possibility)
fastqc $PRJCT_DIR/dmux/raw/* --outdir $PRJCT_DIR/quality/dmux/

## Run in parallel (2nd possibility)
find $RAW_DATA/dmux/raw/* -name '*.fastq.gz' 2>/dev/null | parallel fastqc {1} -o $PRJCT_DIR/quality/dmux/
find $RAW_DATA/raw/* -name '*.fastq' 2>/dev/null | parallel fastqc {1} -o $PRJCT_DIR/quality/

# 3 - Run MULTIqc on FastQC files to integrate results

mkdir $PRJCT_DIR/quality/dmux/multiqc

multiqc --interactive $PRJCT_DIR/quality/dmux --outdir $PRJCT_DIR/quality/dmux/multiqc

# 4 - Now navigate on the files Rstudio pannel to open MULTIqc report on web browser and view results

```

## Raw data pre-processing

### R packages

```{r, eval=FALSE,echo=TRUE}

# 1 - Installing and loading R packages
{
  required_packages <- c("dplyr", "tidyr", "tibble", "stringr", "ggplot2", "ggbreak", "phyloseq", "Biostrings", "Matrix", "ShortRead", "dada2", "DECIPHER", "future", "ggh4x", "vegan", "Rfastp")

for (package in required_packages){
  if(!require(package,character.only = TRUE)) {
    install.packages(Package, dependencies=TRUE)
    }
  library(package,character.only = TRUE)
}
}

# 2 - Cutadapt path
cutadapt <- "/usr/local/bin/cutadapt"
```

## Set output and data paths

Here we will define a single project folder, and the pipeline will create the necessary subfolders for results organization. Only the this main project folder has to be edited on the code bellow.

```{r, eval=FALSE,echo=FALSE, results='hide'}

# 1 - Create and set output and input paths  ----

## Use the same path you created on the bash $PRJCT_DIR variable
analysis_path <- "/home/gabriel/projetos/analises/metagen/MTG05_0924"

## Project name radical
prjct_rad <- "MTG05_0924"  

## Define base paths
data_path <- paste0(analysis_path, "/data")
results_path <- paste0(analysis_path, "/results")

## Create folders 
{
  data_path <- paste0(analysis_path,"/data")
  if(!dir.exists(data_path)){ 
    dir.create(data_path)
  }else{
      print(paste0("The folder ", data_path, " already exists"))
    }
  
  # create quality_folder
  qual_path <- paste0(analysis_path,"/quality")
  if(!dir.exists(qual_path)){ 
    dir.create(qual_path)
  }else{
      print(paste0("The folder ", qual_path, " already exists"))
    }
  
  ## create quality_folder
  multi_path <- paste0(qual_path,"/multiqc")
  if(!dir.exists(multi_path)){ 
    dir.create(multi_path)
  }else{
      print(paste0("The folder ", multi_path, " already exists"))
    }
  
  ## create processed reads folder
  pipe_libs <- paste0(data_path,"/reads")
  if(!dir.exists(pipe_libs)){ 
    dir.create(pipe_libs)
  }else{
      print(paste0("The folder ", pipe_libs, " already exists"))
    }
  
  ## create processed reads folder
  paired_libs <- paste0(data_path,"/reads/paired")
  if(!dir.exists(paired_libs)){ 
    dir.create(paired_libs)
  }else{
      print(paste0("The folder ", paired_libs, " already exists"))
    }
  
  ## create processed reads folder
  cutadapt_libs <- paste0(data_path,"/reads/cutadapt")
  if(!dir.exists(cutadapt_libs)){ 
    dir.create(cutadapt_libs)
  }else{
      print(paste0("The folder ", cutadapt_libs, " already exists"))
    }
  
  ## create results folder
  results_path <- paste0(analysis_path,"/results")
  if(!dir.exists(results_path)){ 
    dir.create(results_path)
  }else{
      print(paste0("The folder ", results_path, " already exists"))
    }
  
  ## create figs folder
  figs_path <- paste0(results_path,"/figs")
  if(!dir.exists(figs_path)){ 
    dir.create(figs_path)
  }else{
      print(paste0("The folder ", figs_path, " already exists"))
    }
  
  ## create blast folder
  blast_path <- paste0(results_path,"/blast")
  if(!dir.exists(blast_path)){ 
    dir.create(blast_path)
  }else{
      print(paste0("The folder ", blast_path, " already exists"))
    }
  
  ## create swarm folder
  swarm_path <- paste0(results_path,"/swarm")
  if(!dir.exists(swarm_path)){ 
    dir.create(swarm_path)
  }else{
      print(paste0("The folder ", swarm_path, " already exists"))
  }
  
    ## create environment folder
  env_path <- paste0(analysis_path,"/environments")
  if(!dir.exists(env_path)){ 
    dir.create(env_path)
  }else{
      print(paste0("The folder ", env_path, " already exists"))
    }

}

list.files(analysis_path)

```

## Load Samples table

```{r, eval=FALSE, echo=FALSE, results='hide'}

# 1 - load primers indexes and samples table

## This is the most important input on the analysis
## Required columns with unique values: File_name

## This table was edited on https://docs.google.com/spreadsheets/d/1Tr5huiIKVF3Bxh7oTMS_vvBv6QN8TqieDci5iOW2zNg/edit#gid=1453953568

primers_n_samples <- primers_n_samples %>% 
  mutate("Unique_File_name" = File_name) %>%
  relocate(Unique_File_name) 

View(primers_n_samples)

# 2 - Test if Sample_names and File_names are unique

## File_names precisam ser unicos
## Sample_names nao precisam ser unicos, podendo permitir agrupamento

if (nrow(primers_n_samples) != length(unique(primers_n_samples$Unique_File_name))) {
  
  print("Your file names are not unique")
  print(paste0("The file names:"))
  print(paste0(primers_n_samples$Unique_File_name[duplicated(primers_n_samples$Unique_File_name)],collapse = "\n "))
  print(paste0("apear more than once"))
  
  } else if (nrow(primers_n_samples) != length(unique(primers_n_samples$Sample))) {
    
  print("Your Sample names are not unique")
  print(paste0("The sample names:"))
  print(paste0(primers_n_samples$Sample[duplicated(primers_n_samples$Sample)],collapse = "\n "))
  print(paste0("apear more than once"))
  }else{
    
  print("All file names and sample names are unique!")
}
      
# 2 - chech if is there any duplicated file name
primers_n_samples %>% nrow()
primers_n_samples$Unique_File_name %>% length()
primers_n_samples$Unique_File_name %>% unique() %>% length()
primers_n_samples$Unique_File_name %>% duplicated() %>% which()
primers_n_samples$Unique_File_name[primers_n_samples$Unique_File_name %>% duplicated()]
```

## Identify file name names radicals

Here we set the raw reads files and the .csv table containing the informations about sample name, primers, indexes, controls and any other metadata, such as local of collection, sampling dates, replicates, volume, weather conditions, etc.

This table must have the columns **Sample**, **Primer** and **Unique_File_name**. This last one must identify uniquely the samples, with the prefix radicals correspondent to their respective R1 and R2 read files.

```{r, eval=FALSE, echo=FALSE, results='hide'}

## Verify Unique_file_names!
primers_n_samples %>% 
  arrange(Primer, File_name) %>% 
  pull(Unique_File_name) %>% 
  paste0(collapse = '","') %>% 
  cat()

primers_n_samples$Unique_File_name %>% 
  paste0(collapse = '","') %>% cat()

## Arrange primers_n_samples by Sample
primers_n_samples <- primers_n_samples %>%
  arrange(Sample)

View(primers_n_samples)

## Organize sample names
{
  duplicated(primers_n_samples$Sample) %>% 
    sum()
  
  sample_levels <- c(primers_n_samples$Sample) %>% 
    unique()

}

## Verify Sample names!
sample_levels

```

## DADA2

### Identify files

To avoid merging reads that did no come from the same cluster, we must check read pairing and remove unpaired (mandatory on DADA2).

```{r, eval=FALSE, echo=FALSE, results='hide'}

# 1 - Carregando os arquivos R1 e R2

## Verificar o formato dos arquivos!
primers_n_samples$`Raw data path` %>% unique() %>% list.files()

## Obtendo os caminhos dos arquivos pareados
read_files <- primers_n_samples %>% 
  select(File_name,`Raw data path`) %>% 
  mutate("Read file"  = paste0(`Raw data path`,"/", File_name,"*")) %>%
  pull("Read file") 

## Salvando os arquivos R1
all_fnFs <- sort(list.files(dirname(read_files), 
                            pattern="_R1_001.fastq|_R1_001.fastq.gz|\\.1.fastq|\\.R1.fastq|\\R1.fq|\\R1.fastq|1.fq.gz|-R1--raw.fastq.gz", full.names = TRUE))

## Salvando os arquivos R2
all_fnRs <- sort(list.files(dirname(read_files),  
                            pattern = "_R2_001.fastq|_R2_001.fastq.gz|\\.2.fastq|\\.R2.fastq|\\R2.fq|\\R2.fastq|2.fq.gz|-R2--raw.fastq.gz", full.names = TRUE))

## O tamanho tem que ser o mesmo!
length(all_fnFs)
length(all_fnRs)

# 2 - Loading sample data (origin and indexes)

## Verificar o que e' pra manter
View(primers_n_samples)

sample_idx_tbl_wide <- primers_n_samples %>% 
  dplyr::select(c("Unique_File_name", 
                  "File_name", 
                  "Lib",
                  # "Primer FWD_sequence", 
                  # "Primer REV_sequence",
                  "Run",
                  "Project",
                  "Researcher",
                  "Sample",
                  # "Primer FWD_name",
                  # "Primer REV_name",
                  "Demultiplexed",
                  "Raw data path",
                  "Primer",
                  "Type",
                  # "Lib name",
                  # starts_with ("metadata"),
                  # "Extraction control",
                  "PCR control"
                  # "Filtration control"
                  # ends_with("control")
                  ))  %>%
  mutate("FWD_R1" = "F-R1",
         "FWD_R2" = "F-R2")

View(sample_idx_tbl_wide)

## Atencao!! alterar o formato dos arquivos, se o separador e' _R1 _R2 ou -R1 -R2

for (sample in 1:nrow(sample_idx_tbl_wide)) {
  
  print(sample_idx_tbl_wide$File_name[sample])
  sample_idx_tbl_wide$FWD_R1[sample] <- 
    all_fnFs[grep(pattern =
                    paste0(sample_idx_tbl_wide$`Raw data path`[sample], "/",
                           sample_idx_tbl_wide$`Unique_File_name`[sample],"-"),
                  x = all_fnFs)] %>%
    unique()
    
  print(all_fnFs[grep(pattern =  
                        paste0(sample_idx_tbl_wide$`Raw data path`[sample], "/", 
                               sample_idx_tbl_wide$`Unique_File_name`[sample],"-"),
                      x = all_fnFs)])
  sample_idx_tbl_wide$FWD_R2[sample] <-  
    all_fnRs[grep(pattern =
                    paste0(sample_idx_tbl_wide$`Raw data path`[sample],
                           "/", 
                           sample_idx_tbl_wide$`Unique_File_name`[sample],"-"), 
                  x = all_fnRs)] %>%
    unique()
  
  print(all_fnRs[grep(pattern =  paste0(sample_idx_tbl_wide$`Raw data path`[sample], "/",
                                        sample_idx_tbl_wide$`Unique_File_name`[sample],"-"),
                      x = all_fnRs)])
  
}

# 3 - Map sample names to reads files ----

sample_idx_tbl_wide %>% colnames()

sample_idx_tbl <- sample_idx_tbl_wide %>% 
  mutate("FWD_R1_paired" = paste0(data_path,"/reads/paired/",Unique_File_name,"-FWD_R1_paired.fastq.gz"),
         "FWD_R2_paired" = paste0(data_path,"/reads/paired/",Unique_File_name,"-FWD_R2_paired.fastq.gz")) %>% 
  pivot_longer(cols = c(
    "FWD_R1", "FWD_R2", "FWD_R1_paired", "FWD_R2_paired"),
               names_to = "Stage",
               values_to = "Read file") %>%
  mutate("Unique_File_name_Primer" = Unique_File_name)

View(sample_idx_tbl)

sample_idx_tbl$`Read file` %>% unique() %>% sort()

```

### Identify primers on the original sequences

```{r, eval=FALSE, echo=FALSE, results='hide'}
#1 - identify primers ----

sample_idx_tbl$Primer %>% unique() %>% str_split(pattern = ";")

#primers sequences used for each sample

# Cutadapt  accepts IUPAC code: https://www.bioinformatics.org/sms/iupac.html

# Name primers: XXXxxXXXX_FWD or XXXxx-XXXX_REV

# Primers analises Heron 
{
# COIr1 ----
# COIr1_FWD <- "TCHACHAAYCAYAARGAYATYGG" #MG2-LCO1490_F
# names(COIr1_FWD) <- "COIr1_FWD"
# COIr1_REV <- "ACYATRAARAARATYATDAYRAADGCRTG"    #MG2-univ-R1
# names(COIr1_REV) <- "COIr1_REV"
# 
# COIr2 ----
# COIr2_FWD <- "TCHACHAAYCAYAARGAYATYGG" #MG2-LCO1490_F
# names(COIr2_FWD) <- "COIr2_FWD"
# COIr2_REV <- "ARTCARTTWCCRAAHCCHCC"    #fwhR1_R2
# names(COIr2_REV) <- "COIr2_REV"
# COIr2b_REV <- "AYNARTCARTTHCCRAAHCC"   #CO1-CFMR-dege_R3
# names(COIr2b_REV) <- "COIr2b_REV"

# ARTCARTTWCCRAAHCCHCC
# AYNARTCARTTHCCRAAHCC
# 
# TM primers ----
# p16S ----
# p16S_FWD <- "CCTAYBBBRBGCASCAG"
# names(p16S_FWD) <- "p16S_FWD"
# p16S_REV <- "GGACTACNNGGGTATCTAAT"
# names(p16S_REV) <- "p16S_REV"

# pITS ----
# pITS_FWD <- "CTTGGTCATTTAGAGGAAGTAA"
# names(pITS_FWD) <- "pITS_FWD"
# pITS_REV <- "GCTGCGTTCTTCATCGATGC"
# names(pITS_REV) <- "pITS_REV"
# # 
# 
# 

# NeoFish ----
# neo_FWD <- "CGCCGTCGCAAGCTTACCCT"
# names(neo_FWD) <- "neo_FWD"
# neo_REV <- "AGTGACGGGCGGTGTGTGC"
# names(neo_REV) <- "neo_REV"
# 
# 12S-MiBird ----
# mBir_FWD <- "GGGTTGGTAAATCTTGTGCCAGC"
# names(mBir_FWD) <- "mBir_FWD"
# mBir_REV <- "CATAGTGGGGTATCTAATCCCAGTTTG"
# names(mBir_REV) <- "mBir_REV"
# 
# 16S-BF2 ----
# BF2_FWD <- "TGCATCGGTTGGGGTGACCTCGGA"
# BF2_FWD <- "CGGTTGGGGTGACCTCGGA"
# names(BF2_FWD) <- "BF2_FWD"
# # BF2_REV <- "TGCATGCTGTTATCCCTAGGGTAACT"
# BF2_REV <- "GCTGTTATCCCTAGGGTAACT"
# names(BF2_REV) <- "BF2_REV"

# fwh2 (COI) ----
# fwh2_FWD <- "GGDACWGGWTGAACWGT"
# names(fwh2_FWD) <- "fwh2_FWD"
# fwh2_REV <- "GTRATWGCHCCDGCTARWACWGG"
# names(fwh2_REV) <- "fwh2_REV"

# MiFish ----
  
# for i in `ls MTG05*`; do zcat ${i} | grep -c "GTCGGTAAAACTCGTGCCAGC"; done
# for i in `ls MTG05*`; do zcat ${i} | grep -c "TGCATCATAGTGGGGTATCTAATCCCAGTTG"; done
# for i in `ls MTG05*`; do zcat ${i} | grep -c "CATAGTGGGGTATCTAATCCCAGTTTG"; done
# for i in `ls MTG05*`; do zcat ${i} | grep -c "CATAGTGGGGTATCTAATCCCAGTTG"; done


# MiFish_FWD <-     "GTCGGTAAAACTCGTCCAGC"
# MiFish_FWD <-   "GTCGGTAAAACTCGTGCCAGC" # versão Ecomol Paranaíba e MiFish RJ & SP e Juliana q tem um G a mais no meio ????
# names(MiFish_FWD) <- "MiFish_FWD"
# MiFish_REV <-  "TGCATCATAGTGGGGTATCTAATCCCAGTTG"
# MiFish_REV <-     "CATAGTGGGGTATCTAATCCCAGTTTG" # versão Ecomol Paranaíba e MiFish RJ & SP e Juliana
# MiFish_REV <-   "CATAGTGGGGTATCTAATCCCAGTTG" #versão reduzida

  ## Versao MetaGen MTG05_0924

  MiFish_FWD <-   "GCCGGTAAAACTCGTGCCAGC"
  names(MiFish_FWD) <- "MiFish_FWD"
  MiFish_REV <-   "CATAGTGGGGTATCTAATCCCAGTTTG"
  names(MiFish_REV) <- "MiFish_REV"

# names(MiFish_REV) <- "MiFish_REV"
#  
# #COI-1 ----
# C1_FWD <- "GGWACWGGWTGAACWGTWTAYCCYCC"
# names(C1_FWD) <- "C1_FWD"
# # C1_REV <- "TAIACYTCIGGRTGICCRAARAAYCA"
# C1_REV <-   "TADACYTCDGGRTGDCCRAARAAYCA"
# names(C1_REV) <- "C1_REV"
# 
# #COI-3 ----
# C3_FWD <- "ACYAAICAYAAAGAYATIGGCAC"
# C3_FWD <-   "ACYAADCAYAAAGAYATDGGCAC"
# names(C3_FWD) <- "C3_FWD"
# # C3_REV <- "CTTATRTTRTTTATICGIGGRAAIGC"
# C3_REV <-   "CTTATRTTRTTTATDCGDGGRAADGC"
# names(C3_REV) <- "C3_REV"
# 
# #COI-Inseto ----
# CI_FWD <- "GGTACATTCAACCAATCATAAAGATATTGG"
# names(CI_FWD) <- "CI_FWD"
# CI_REV <- "GGGTACCGTGGAAAWGCTATATCWGGTG"
# names(CI_REV) <- "CI_REV"
# 
# #COI-Lep (quase igual COI-Inseto) ----
# Clep_FWD <- "ATTCAACCAATCATAAAGATATTGG"
# names(Clep_FWD) <- "Clep_FWD"
# Clep_REV <- "CGTGGAAAWGCTATATCWGGTG"
# names(Clep_REV) <- "Clep_REV" 

# MiMammal-U-12S ----
  #                  https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-11-434/tables/1
# mmu_FWD <- "GGGTTGGTAAATTTCGTGCCAGC"
# names(mmu_FWD) <- "mmu_FWD"
# mmu_REV <- "CATAGTGGGGTATCTAATCCCAGTTTG"
# names(mmu_REV) <- "mmu_REV"

# #Elas02 ----
# Elasm02_FWD <- "GTTGGTHAATCTCGTGCCAGC"
# names(Elasm02_FWD) <- "Elasm02_FWD"
# Elasm02_REV <- "CATAGTAGGGTATCTAATCCTAGTTTG"
# names(Elasm02_REV) <- "Elasm02_REV"
# 
# #Fish1 ----
# Fish1_FWD <- "TCAACCAACCACAAAGACATTGGCAC"
# names(Fish1_FWD) <- "Fish1_FWD"
# Fish1_REV <- "TAGACTTCTGGGTGGCCAAAGAATCA"
# names(Fish1_REV) <- "Fish1_REV"
# # 
# #Fish2 ----
# Fish2_FWD <- "TCGACTAATCATAAAGATATCGGCAC"
# names(Fish2_FWD) <- "Fish2_FWD"
# Fish2_REV <- "ACTTCAGGGTGACCGAAGAATCAGAA"
# names(Fish2_REV) <- "Fish2_REV"
# # 
# # #VF2_FR1d ----
# VF2_FR1d_FWD  <- "CAACCAACCACAAAGACATTGGCAC"
# names(VF2_FR1d_FWD) <- "VF2_FR1d_FWD"
# VF2_FR1d_REV <- "ACCTCAGGGTGTCCGAARAAYCARAA"
# names(VF2_FR1d_REV) <- "VF2_FR1d_REV"

# # 12S Vertebrados ----
# zgrep -c "TTAGATACCCCACTATGC" *V5*gz
# ITS2_FWD <- "TTAGATACCCCACTATGC" # sempre conferir onde o FWD está sendo encontrado
# names(ITS2_FWD) <- "ITS2_FWD"
# ITS2_REV <- "TAGAACAGGCTCCTCTAG"
# names(ITS2_REV) <- "ITS2_REV"
    #https://academic.oup.com/nar/article/39/21/e145/1105558?login=true   ORIGINAL Riaz et al. 2011, tb está trocado FWD REV
    #https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13485    #neste o 12SV5 está trocado FWD REV

# 
# CAACCAACCACAAAGACATTGGCAC
# CAACCAACCACAAAGACATTGGCACCCTTTACTTAGTATTCGGTGCCTGAGCTGGAATAGTAGGCACAGCTCTCAGCCTCCTAATCCGAGCAGAACTAAGTCAACCTGGCTCCCTGCTAGGTGATGATCAAATCTACAATGTTATCGTAACTGCACATGCATTTGTGATAATTTTCTTTATAGTAATACCAGTAATGATTGGGGGCTTCGGAAACTGACTTATTCCCCTAATGATCGGTG
# 
# library(bold)
# bold_res <- bold_identify(sequences = c("CCTTTACTTAGTATTCGGTGCCTGAGCTGGAATAGTAGGCACAGCTCTCAGCCTCCTAATCCGAGCAGAACTAAGTCAACCTGGCTCCCTGCTAGGTGATGATCAAATCTACAATGTTATCGTAACTGCACATGCATTTGTGATAATTTTCTTTATAGTAATACCAGTAATGATTGGGGGCTTCGGAAACTGACTTATTCCCCTAATGATCGGTG"),
#               db = c("COX1", "COX1_SPECIES", "COX1_SPECIES_PUBLIC", "COX1_L640bp"),
#               response = FALSE,
#               keepSeq = TRUE)
# 
# View(bold_res[[1]])
# str(bold_res)
# 
# bold::bold_identify_parents(bold_res, taxOnly = TRUE)

# 
# #trnL (UAA) - Plantas  ----
# trnLg_F <- "GGGCAATCCTGAGCCAA"
# names(trnLg_F) <- "trnLg_F"
# trnLh_R <- "CCATTGAGTCTCTGCACCTATC"
# names(trnLh_R) <- "trnLh_R"

# Mini_SH_E  (Mistura do COI-3-fwd com o MiFish-rev) ----
# Mini_SH_E_F <- "ACYAAICAYAAAGAYATIGGCAC"
# Mini_SH_E_F <- "ACYAADCAYAAAGAYATDGGCAC"
# names(Mini_SH_E_F) <- "Mini_SH_E_F"
# Mini_SH_E_R <- "CATAGTGGGGTATCTAATCCCAGTTG"
# names(Mini_SH_E_R) <- "Mini_SH_E_R"
# 
# 16S MAV - Invertebrados  ----
# p16SMAV_F <- "CCAACATCGAGGTCRYAA"
# names(p16SMAV_F) <- "p16SMAV_F"
# p16SMAV_R <- "ARTTACYNTAGGGATAACAG"
# names(p16SMAV_R) <- "p16SMAV_R"
# 
# COI_minibar ----
# minibar_F1 <- "TCCACTAATCACAARGATATTGGTAC"
# names(minibar_F1) <- "minibar_F1"
# minibar_R1 <- "GAAAATCATAATGAAGGCATGAGC"
# names(minibar_R1) <- "minibar_R1"
# # 
# COI-ZBJ-Art ----
# # COIZBJart_F1c
# COIZBJart_FWD <- "AGATATTGGAACWTTATATTTTATTTTTGG"
# names(COIZBJart_FWD) <- "COIZBJart_FWD"
# # COIZBJart_R2c
# COIZBJart_REV <- "WACTAATCAATTWCCAAATCCTCC"
# names(COIZBJart_REV) <- "COIZBJart_REV"
# # 
# Mini COI ----
# miniCOI_FWD <- "ATCACCACTATTGTTAATATAAAACCC"
# names(miniCOI_FWD) <- "miniCOI_FWD"
# miniCOI_REV <- "TAAACCTCAGGATGTCCGAAGAATCA"
# names(miniCOI_REV) <- "miniCOI_REV"
}

# Primers analises MetaGen
# ITS2 ----
# ITS2_FWD <- "ATGCGATACTTGGTGTGAAT"
# names(ITS2_FWD) <- "ITS2_FWD"
# ITS2_REV <- "GACGCTTCTCCAGACTACAAT"
# names(ITS2_REV) <- "ITS2_REV"

# BF2 (COI) ----
# BF2_FWD <- "GCHCCHGAYATRGCHTTYCC"
# names(BF2_FWD) <- "BF2_FWD"
# BF2_REV <- "TCDGGRTGNCCRAARAAYCA"
# names(BF2_REV) <- "BF2_REV"

 # creates a list of single row tibbles for each primer ----
primers <- tibble("Primer seq" = c(
  # ITS2_FWD, ITS2_REV # Primers ITS
  MiFish_FWD, MiFish_REV 
)) %>% 
  dplyr::mutate("Primer name"= names(`Primer seq`)) %>% 
  split(1:nrow(.)) 

primers
```

### Generate sequences for complement, reverse and reverse complement of each primer

The function *allOrients* is used to generate all possible orientations for primers FWD e REV.

```{r, eval=FALSE, echo=FALSE, results='hide'}
# 1 - generate all possible primer orientations ----

#function to get all possible primer orientations ----

allOrients <- function(primers) {
   # Create all orientations of the input sequence
    # Must be a tibble with cols = c(Primers,`Primer name`)
  
   require(Biostrings)
   dna <- Biostrings::DNAString(primers$`Primer seq`)  # The Biostrings works w/ DNAString objects rather than character vectors
   orients <- c(Forward = dna, 
                Complement = Biostrings::complement(dna), 
                Reverse = Biostrings::reverse(dna),
                RevComp = Biostrings::reverseComplement(dna))
   names(orients) <- paste0(names(orients))
   
   primer_tbl <- sapply(orients, toString)
   
   primer_tbl <- tibble::tibble(Sequence = primer_tbl,
                        `Primer orientation` = base::names(primer_tbl)) %>% 
     dplyr::mutate(`Primer` = primers$`Primer name`) %>%
     tidyr::unite(col=`Orientation name`, `Primer` ,`Primer orientation`,remove = FALSE)
   
   # %>% 
   #   tidyr::pivot_wider( names_from = `Primer orientation`,values_from = Sequence)
   
   return(primer_tbl)  # Convert back to character vector
}

# 2 - Apply function to generate table with all primers orientations possible
primers_all_orients <- purrr::map_dfr(primers, allOrients)

#name the sequences accordingly
base::names(primers_all_orients$Sequence) <- primers_all_orients$`Orientation name`

primers_all_orients <- primers_all_orients %>% mutate(`Primer pair` = str_remove_all(string = Primer,
                                                              pattern = "_.*.$"))
#check naming
primers_all_orients$Sequence
base::names(primers_all_orients$Sequence)
```

### Count primer presence on reads

Before primer removal it is possible to count their presence on the reads. This procedures is carried on independently for each sample. The following example applies to the first samples of each primer sample set.

```{r, eval=FALSE, echo=FALSE, results='hide'}
# 1 - prepare to count primer orientation hits ----

# 1a - Load required functions ----
#function to count primer on each specific library
primerHits <- function(primer, fn) {
   # Counts number of reads in which the primer is found
   nhits <- Biostrings::vcountPattern(primer, 
                                      ShortRead::sread(ShortRead::readFastq(fn)), 
                                      fixed = FALSE,
                                      max.mismatch = 1)
   return(sum(nhits > 0))
}

#function to call primerHits for multiple primers
multi_primerHits <- function(Read_file,primers){
  primer_counts <- purrr::map_df(primers,
                                 .f = primerHits, 
                                 fn = Read_file)
  
  primer_counts <- primer_counts %>% 
    dplyr::mutate(`Read file` = Read_file)
  return(primer_counts)
}

sample_idx_tbl$Stage %>% unique()

# 2b - Create vector of read files to look on for primers ----
reads_seqs_tbl <- sample_idx_tbl %>% 
  filter(Stage %in% c("FWD_R1", "FWD_R2")) %>% 
  dplyr::select(`Read file`,File_name) 

reads_seqs <- reads_seqs_tbl$`Read file`
names(reads_seqs) <- reads_seqs_tbl$File_name

reads_seqs

reads_seqs %>% unique()
reads_seqs %>% lengths()

# 2c - named vector of primer sequences ----
primers_seqs <- primers_all_orients %>% pull(Sequence,name = `Orientation name`)

# 2d - Set up for parallel searching ----
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78

future::plan(future::multisession(workers = cores_to_be_used))

# 3 - Count primers on reads ----
primers_in_Nreads <- furrr::future_map_dfr(reads_seqs,
                                           .f = multi_primerHits, 
                                           primers = primers_seqs, 
                                           .options = furrr::furrr_options(seed = NULL))

# 4 - identify which primers were found on most reeads ----
dim(primers_in_Nreads)

colSums(primers_in_Nreads[,1:length(primers_seqs)]) %>% sort()

# 5 - Save primers in N-cleaned-reads complete table (if desired)

write.csv(x = primers_in_Nreads, file = paste0(results_path,"/",prjct_rad,"-primers_found_in_reads.csv"))

# 6- Remove columns(primers) not found in any sample

#Identify empty counts
colnames(primers_in_Nreads) #choose only numeric columns (primer counts)
colnames(primers_in_Nreads[1:length(primers_seqs)]) #choose only numeric columns (primer counts)
colSums(primers_in_Nreads[1:length(primers_seqs)]) # dá pra excluir do plot se tiver zero counts
(colSums(primers_in_Nreads[1:length(primers_seqs)]) == 0)#transformando em um vetor logico
colnames(primers_in_Nreads)[(colSums(primers_in_Nreads[1:length(primers_seqs)]) != 0)]#transformando em um vetor logico

colnames(primers_in_Nreads)

#get sample information into primers_in_Nreads table
primers_in_Nreads <- left_join(primers_in_Nreads,sample_idx_tbl,by = "Read file") %>% 
  mutate(Read = if_else((str_detect(Stage,
                                    pattern = "R1")),
                        "R1",
                        "R2")) %>% 
  # mutate(Unique_File_name = factor(Unique_File_name,levels = sample_levels)) %>%
  unite(col = "Unique_File_name_Read", sep = " ", remove = FALSE,
        Unique_File_name,Read) 

# count numbers of reads in original RAW files

primers_in_Nreads <- primers_in_Nreads %>% 
  mutate(`Total reads` =  ShortRead::countFastq(dirPath = .$`Read file`)[,1])


#7- prepare primer counts for plots ----

rownames(primers_in_Nreads) <- primers_in_Nreads$Unique_File_name_Read

primers_in_Nreads$`Read file`

```

### Plot primers identified in each sample

```{r, eval=FALSE, echo=FALSE, results='hide'}
#8 - prepare primer counts for plots in ggplot----

primers_all_orients$`Orientation name`

primers_in_Nreads %>% colnames() %>%  paste0(collapse = '",\n') %>% cat()

#convert primer hits table to long format
primers_in_Nreads_long <- primers_in_Nreads %>% 
  gather(key = Sequences, 
         value = Count, 
         ends_with("FWD_Forward"),
         ends_with("FWD_RevComp"),
         ends_with("REV_Forward"),
         ends_with("REV_RevComp")
                  ) %>% 
  mutate(Sequences = as.factor(Sequences),
         Unique_File_name = factor(Unique_File_name,levels = sample_levels)) %>% 
  dplyr::select(-c("Read file","Stage")) %>% 
  select(where(function(x) any(!is.na(x)))) %>% 
  filter(Count != 0) 
  
# PLOT 1: primers counts in readstile plot - only primers FWD & REV, foward & revcomp ----

options(scipen=10000)

library(viridis)
library(ggh4x)

#create levels for Files display on plot
Unique_File_name_Read_levels <- primers_in_Nreads_long %>% 
  # mutate("Metadata 12" = as.numeric(`Metadata 12`)) %>% 
  # arrange(`Metadata 12`) %>% 
  pull(Unique_File_name_Read) %>% unique()

# defining all separate projects to plot ----
all_projects <- unique(primers_in_Nreads_long$Project) %>%
# all_projects <- unique(primers_n_samples$Project) %>% 
  str_split(pattern = ";",simplify = F) %>% 
  unlist() %>% 
  unique()

# Criacao do plot
for (project in all_projects) {

project_name <- project %>% str_replace_all(pattern = " ",
                                        replacement = "_")
N_samples <-   primers_in_Nreads_long %>%
  filter(str_detect(string = Project, pattern = project)) %>% 
  pull(Unique_File_name_Read) %>% unique() %>% length()

primers_tile <- primers_in_Nreads_long %>%
  filter(Project %in% project) %>% 
  mutate(Unique_File_name_Read = factor(Unique_File_name_Read, levels = Unique_File_name_Read_levels)) %>% 
  # ggplot2::ggplot(aes(y = interaction(Unique_File_name,Read,sep = " - "),
  ggplot2::ggplot(aes(y = interaction(Unique_File_name_Read,Sample,sep = " - "),
  # ggplot2::ggplot(aes(y = interaction(Sample,Unique_File_name_Read,sep = " - "),
  # ggplot2::ggplot(aes(y = Sample,
                      x = Sequences,
                      fill = (Count/`Total reads`*100),
                      group =`Primer`,alpha = 0.125)) +
  geom_tile(aes(col = `Primer`), 
            linewidth = 0.125, 
            linetype = 10) +
  geom_text(aes(label = Count), 
            size = 4,
            fontface = "bold") +
  scale_fill_gradientn(name = "Proportion of reads\n     with primer (%)",
                       colours = c("white","red","yellow","green","dark green"),
                       values = c(0,1),
                       na.value ="white") +
  # scale_colour_manual(values = c("#233fdb","#ff3455","#2c9400")) +
  # scale_colour_manual(values = viridis::turbo(n = length(unique(primers_in_Nreads_long$Sequences))/8)) +
  guides(color = guide_legend(override.aes = list(fill = "white", 
                                                  size = 25))) +
  theme_light(base_line_size = 0.0625,
              base_size = 15) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  geom_vline(xintercept = c(4.5,8.5,12.5,16.5,20.5,24.5),color = "black") +
  geom_hline(yintercept = seq(0.5,300,2),color = "darkgrey") +
  xlab("Primers") +
  ylab("Amostra") +
  scale_y_discrete(limits=rev) +
  # ggtitle(label = paste0("Ecomol - ",prjct_rad, " - ", project),
  ggtitle(label = paste0("GMB - ", project),
              subtitle = "Presença de primers nas amostras:\nIntensidade de cor relativa à contagem do respectivo primer no conjunto de reads\n(max. mismatch = 1)") +
  theme(strip.text.y = element_text(size = 25),
        plot.title = element_text(size=25),
        plot.subtitle = element_text(size=15),
        axis.text.x = element_text(size=15),
        legend.text= element_text(size=12.5),
        legend.title = element_text(size=17.5))+ scale_alpha(guide = 'none') +
  # facet_grid(rows = vars(Researcher,Primer),
  facet_grid(rows = vars(Primer),
             scales = "free",
             space = "free")

primers_tile


ggsave(file = paste0(figs_path,"/",project_name,"-primers_found_in_reads.pdf"),
     plot = primers_tile,
     device = "pdf",
     width = 45,
     height = max(N_samples/5*3.5,10),
     units = "cm",
     dpi = 600)
}

#once generated, it is necessary to check if the FWD and REV primers orientation is correct.
#   if not, change the script as in:    https://benjjneb.github.io/dada2/ITS_workflow.html

# When the amplicon is shorter than the reads is expected the FWD primer
#  to be found in the forward reads in its forward orientation,
#    and in some of the reverse reads in its reverse-complement orientation
#    (due to read-through when the ITS region is short).
# Similarly the REV primer is found with its expected orientations.
#
# Note: Orientation mixups are a common trip-up. If, for example,
#    the REV primer is matching the Reverse reads in its RevComp orientation,
#    then replace REV with its reverse-complement orientation
# (REV <- REV.orient[["RevComp"]]) before proceeding.
```

### Repairing reads

We will now generate file paths for the reads that will be repaired with DADA2

```{r, eval=FALSE, echo=FALSE, results='hide'}

primer_paired_files <- sample_idx_tbl %>%
    filter(str_detect(string = Stage,
                      pattern = "FWD_R1|FWD_R2|FWD_R1_paired|FWD_R2_paired")) %>% 
    pivot_wider(id_cols = c("Unique_File_name","Primer","Lib"),
                names_from = "Stage",
                values_from = "Read file") 

primer_paired_files

primer_paired_files$FWD_R1 %>% BiocGenerics::basename()
primer_paired_files$FWD_R1_paired %>% basename()

# Repairing in parallell ----

# Function to repair and filter reads after demultiplexing (from CDI or not)

repairNfilt_with_DADA2 <- function(Unique_File_name,
                                   FWD_R1_in,
                                   FWD_R2_in,
                                   FWD_R1_out,
                                   FWD_R2_out){
  #FWD oriented amplicon
  all_filtered_out <- tibble()

      print(paste0("      FWD orientation"))
      print(paste0("Working on file: ",Unique_File_name))
            sample_filtered_out <- tibble()
      sample_filtered_out <- dada2::filterAndTrim(
        fwd = FWD_R1_in,
        filt = FWD_R1_out,
        rev = FWD_R2_in,
        filt.rev = FWD_R2_out,
        maxN = c(0,0),
        maxEE = c(30,30),
        multithread = T,
        matchIDs = TRUE,
        rm.phix = TRUE,
        compress = TRUE,
        minLen = 30,
        verbose = TRUE)

      sample_filtered_out <- sample_filtered_out %>%
        as_tibble() %>%
        mutate("Unique_File_name" = Unique_File_name
               )

  all_filtered_out <- bind_rows(all_filtered_out,sample_filtered_out)

  return(all_filtered_out)
}

## Usando a funcao ----

# Versões paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78
future::plan(future::multisession(workers = cores_to_be_used))

{
  tictoc::tic()

all_filtered_out_fun <- repairNfilt_with_DADA2(
  Unique_File_name = primer_paired_files$Unique_File_name,
  FWD_R1_in = primer_paired_files$FWD_R1,
  FWD_R2_in = primer_paired_files$FWD_R2,
  FWD_R1_out = primer_paired_files$FWD_R1_paired,
  FWD_R2_out = primer_paired_files$FWD_R2_paired)

tictoc::toc()
}

all_filtered_out <- all_filtered_out_fun %>% 
  mutate(prop = round((reads.out/reads.in*100),digits = 2)) %>% 
  dplyr::rename("Raw reads (pairs)" = "reads.in",
         "Paired reads"  = "reads.out",
          "Proportion" = "prop")

all_filtered_out$Unique_File_name %>% unique()

View(all_filtered_out)

# Save environment
base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_paired.RData"))

```

## Cutadapt - Primer removal

The ***cutadapt*** software ([DOI:10.14806/ej.17.1.200](http://journal.embnet.org/index.php/embnetjournal/article/view/200)) was used for primer removal on read sequences.

### Generate and execute primer-specific commands

```{r eval=FALSE}
# optional: remove all primers from all reads and samples ----

sample_idx_tbl$Stage %>% unique()
sample_idx_tbl$`Read file` %>% unique()

#10 - map sample names to cutadapt reads files ----

#name outputs
paired_files_per_primer <- sample_idx_tbl  %>% 
  filter(Stage %in% c("FWD_R1_paired","FWD_R2_paired"))
  
cut_files_per_primer <- paired_files_per_primer %>%  
  mutate(Stage = gsub(Stage, pattern = "_paired",replacement = "_cutadapt")) %>% 
  mutate("Read file" = str_replace_all(string = `Read file`,
                                       pattern = "paired",
                                       replacement ="cutadapt")) 

# apenas se for separar por primer ----  #####################################################

cut_files_per_primer <- cut_files_per_primer %>% 
separate_longer_delim(cols = "Primer",     
                      delim = ";") %>% 
mutate("Read file" = case_when(Stage %in% c("FWD_R1_cutadapt","FWD_R2_cutadapt") ~
                                  str_replace_all(string = `Read file`,
                                                  pattern = paste0(cutadapt_libs,"/"),
                                                  replacement = paste0(cutadapt_libs,"/",.$Primer,"--")),
                                TRUE ~ `Read file`)) %>%
  unite(col = "Unique_File_name_Primer",
        Primer,Unique_File_name,
        remove = F,
        sep = "--")
  
cut_files_per_primer <- bind_rows(cut_files_per_primer, 
                                  paired_files_per_primer)


#create cutadapt flags from identified primers
#all -----
  primers_all_orients$Primer %>% unique()

#remove primers and filter only the reads that contain the expected primer ----
#prepare cutadapt flags specific for each primer

# ITS2 ----
{
# select the respective orientations found to create cutadapt flags
 ITS2_FWD.orients <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                    grep(pattern = c("ITS2_FWD_Forward"))]

 ITS2_FWD.RC <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                               grep(pattern = c("ITS2_FWD_RevComp"))]

 ITS2_REV.orients <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                    grep(pattern = c("ITS2_REV_Forward"))]

 ITS2_REV.RC <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                               grep(pattern = c("ITS2_REV_RevComp"))]

# creat flags
 # Trim FWD and the reverse-complement of REV off of R1 (forward reads)
 ITS2_R1.flags <- paste0("-g ", ITS2_FWD.orients, " -a ", ITS2_REV.RC)
 # Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
 ITS2_R2.flags <- paste0("-G ", ITS2_REV.orients, " -A ", ITS2_FWD.RC)

 ITS2_R1.flags
 ITS2_R2.flags

}

# name the vector of files to not mix samples
# base::names(sample_idx_tbl$`Read file`) <- sample_idx_tbl$Unique_File_name\    #não da pois tem R! e R2 e os nome se repetem

# ITS2 ----
{
  primer_cut_files <- cut_files_per_primer %>% 
    filter(str_detect(string = Primer,pattern = "ITS")) %>% 
    pivot_wider(id_cols = c("Unique_File_name","Unique_File_name_Primer","Primer"),
                names_from = "Stage",
                values_from = "Read file") 
  
ITS2_fnFs.cut <- primer_cut_files$FWD_R1_cutadapt %>% 
  na.exclude() %>% 
  as.character()

ITS2_fnRs.cut <- primer_cut_files$FWD_R2_cutadapt %>% 
  na.exclude() %>% 
  as.character()

ITS2_fnFs.paired <- primer_cut_files$FWD_R1_paired %>% 
  na.exclude() %>% 
  as.character()

ITS2_fnRs.paired <- primer_cut_files$FWD_R2_paired %>% 
  na.exclude() %>% 
  as.character()

}

# MiFish ----
{
# select the respective orientations found to create cutadapt flags
 MiFish_FWD.orients <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                    grep(pattern = c("MiFish_FWD_Forward"))]

 MiFish_FWD.RC <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                               grep(pattern = c("MiFish_FWD_RevComp"))]

 MiFish_REV.orients <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                    grep(pattern = c("MiFish_REV_Forward"))]

 MiFish_REV.RC <- primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                               grep(pattern = c("MiFish_REV_RevComp"))]

# creat flags
 # Trim FWD and the reverse-complement of REV off of R1 (forward reads)
 MiFish_R1.flags <- paste0("-g ", MiFish_FWD.orients, " -a ", MiFish_REV.RC)
 # Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
 MiFish_R2.flags <- paste0("-G ", MiFish_REV.orients, " -A ", MiFish_FWD.RC)

 MiFish_R1.flags
 MiFish_R2.flags

}

# MiFish ----
{
  primer_cut_files <- cut_files_per_primer %>% 
    filter(str_detect(string = Primer,pattern = "MiFish")) %>% 
    pivot_wider(id_cols = c("Unique_File_name","Unique_File_name_Primer","Primer"),
                names_from = "Stage",
                values_from = "Read file") 
  
MiFish_fnFs.cut <- primer_cut_files$FWD_R1_cutadapt %>% 
  na.exclude() %>% 
  as.character()

MiFish_fnRs.cut <- primer_cut_files$FWD_R2_cutadapt %>% 
  na.exclude() %>% 
  as.character()

MiFish_fnFs.paired <- primer_cut_files$FWD_R1_paired %>% 
  na.exclude() %>% 
  as.character()

MiFish_fnRs.paired <- primer_cut_files$FWD_R2_paired %>% 
  na.exclude() %>% 
  as.character()

}
```

#### Identify primer specific read files

```{r eval=FALSE}

# Run Cutadapt

# ITS2 ----
for(i in seq_along(ITS2_fnFs.cut)) {
  system2(cutadapt, args = c(ITS2_R1.flags,
                             ITS2_R2.flags, 
                             "-j 70", "-n", 3, # -n 2 required to remove FWD and REV from reads
                             "-o", ITS2_fnFs.cut[i], # output files
                             "-p", ITS2_fnRs.cut[i], 
                             ITS2_fnFs.paired[i],  # input files
                             ITS2_fnRs.paired[i],
                             "--minimum-length 10 --discard-untrimmed --match-read-wildcards --pair-filter=both"
                             )) # guarantee no zerolength reads
}

# MiFish ----
for(i in seq_along(MiFish_fnFs.cut)) {
  system2(cutadapt, args = c(MiFish_R1.flags,
                             MiFish_R2.flags, 
                             "-j 70", "-n", 3, # -n 2 required to remove FWD and REV from reads
                             "-o", MiFish_fnFs.cut[i], # output files
                             "-p", MiFish_fnRs.cut[i], 
                             MiFish_fnFs.paired[i],  # input files
                             MiFish_fnRs.paired[i],
                             "--minimum-length 10 --discard-untrimmed --match-read-wildcards --pair-filter=both"
                             )) # guarantee no zerolength reads
  }
```

### Repairing

To avoid merging reads that did no come from the same cluster, we must check read pairing and remove unpaired (mandatory on DADA2).

```{r, eval=FALSE,echo=TRUE}

repair_files_per_primer <- cut_files_per_primer %>% 
   filter(str_detect(string = Stage , pattern = "_cutadapt")) %>% 
    pivot_wider(id_cols = c("Unique_File_name","Unique_File_name_Primer","Primer"),
                names_from = "Stage",
                values_from = "Read file") %>% 
   mutate("FWD_R1_repaired" = str_replace_all(string = FWD_R1_cutadapt, 
                                              pattern = "cutadapt",
                                              replacement = "repaired"),
          "FWD_R2_repaired" = str_replace_all(string = FWD_R2_cutadapt, 
                                              pattern = "cutadapt",
                                              replacement = "repaired")) 

file.exists(repair_files_per_primer$FWD_R1_cutadapt)
file.exists(repair_files_per_primer$FWD_R2_cutadapt)
file.exists(repair_files_per_primer$FWD_R1_repaired)
file.exists(repair_files_per_primer$FWD_R2_repaired)

{
  # Versões paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78
future::plan(future::multisession(workers = cores_to_be_used))

tictoc::tic()
all_filtered_out_fun_repaired <- repairNfilt_with_DADA2(Unique_File_name = repair_files_per_primer$Unique_File_name_Primer,
                                              FWD_R1_in = repair_files_per_primer$FWD_R1_cutadapt,
                                              FWD_R2_in = repair_files_per_primer$FWD_R2_cutadapt,
                                              FWD_R1_out = repair_files_per_primer$FWD_R1_repaired,
                                              FWD_R2_out = repair_files_per_primer$FWD_R2_repaired)
tictoc::toc()
}

all_repaired <- all_filtered_out_fun_repaired %>% 
  mutate(prop = round((reads.out/reads.in*100),digits = 2)) %>% 
  BiocGenerics::unique() %>%
  dplyr::rename("Primer detected reads" = "reads.in",
         "Repaired reads"  = "reads.out",
          "Proportion" = "prop") %>% 
  dplyr::rename("Unique_File_name_Primer" = "Unique_File_name") %>% 
  mutate("Unique_File_name" = str_remove_all(Unique_File_name_Primer,pattern = "^*.*--"))

View(all_repaired)

```

<br>

### Identify error rates intrinsic to sequencing

```{r, eval=FALSE}
# 13 - learn error rates ----

# must be performed independently for diferent sequencing runs !!

View(primer_paired_files)

all_fnFs.repaired <- repair_files_per_primer %>%
                filter(file.exists(FWD_R1_repaired)) %>%
                  pull(FWD_R1_repaired)

names(all_fnFs.repaired) <- repair_files_per_primer %>%
                filter(file.exists(FWD_R1_repaired)) %>%
                  # pull(Unique_File_name_Primer)
                  pull(Unique_File_name_Primer)


all_fnRs.repaired <- repair_files_per_primer %>%
                filter(file.exists(FWD_R2_repaired)) %>%
                  pull(FWD_R2_repaired)

names(all_fnRs.repaired) <- repair_files_per_primer %>%
                filter(file.exists(FWD_R2_repaired)) %>%
                  # pull(Unique_File_name_Primer)
                  pull(Unique_File_name_Primer)

length(all_fnFs.repaired)
length(all_fnRs.repaired)

{
  # errors in R1 reads ----
  all_errF <- learnErrors(fls = all_fnFs.repaired,
                               multithread=TRUE,randomize = TRUE)
# errors in R2 reads ----
  all_errR <- learnErrors(fls = all_fnRs.repaired,
                               multithread=TRUE,randomize = TRUE)

}

primers_n_samples$Lib %>% unique()

```

### Dereplication: grouping into ASVs

On this step each library is reduced to its unique composing sequences and their counts.

```{r, eval=FALSE}
# 14 - DADA2 dereplication ----

# Careful, if not demuxed, reads must come from cutadapt files, not paired

# #all ----
all_derep_forward <- derepFastq(all_fnFs.repaired, verbose=TRUE)

all_derep_reverse <- derepFastq(all_fnRs.repaired, verbose=TRUE)

all_derep_forward[3]
all_derep_reverse[3]

all_dadaFs <- dada(all_derep_forward, err=all_errF, multithread=TRUE)
all_dadaRs <- dada(all_derep_reverse, err=all_errR, multithread=TRUE)

```

### Merge read pairs

On this step the forward an reverse reads are merged, by overlap, in order to reconstruct the insert full sequence.

```{r, eval=FALSE}

length(all_dadaFs)
length(all_dadaRs)
length(all_derep_forward)
length(all_derep_reverse)

# 15 - merge read pairs ----

# Merging ----

all_mergers <- mergePairs(dadaF = all_dadaFs,
                          derepF =  all_derep_forward,
                          dadaR =  all_dadaRs,
                          derepR =  all_derep_reverse,
                          minOverlap = 10,
                          maxMismatch = 1,
                          # returnRejects = TRUE,
                          # justConcatenate = TRUE,
                          verbose=TRUE)
        
mergers_seqtab <- makeSequenceTable(samples = all_mergers)

mergers_seqtab %>% colnames()

primers_all_orients

# use reads R1 and R2 separetely ----

R1_seqtab <- makeSequenceTable(samples = all_dadaFs)
R2_seqtab <- makeSequenceTable(samples = all_dadaRs)
 
dada2:::is.sequence.table(mergers_seqtab)
dada2:::is.sequence.table(R1_seqtab)
dada2:::is.sequence.table(R2_seqtab)

colnames(mergers_seqtab)
colnames(R1_seqtab)
colnames(R2_seqtab)
 
dim(mergers_seqtab)
dim(R1_seqtab)
dim(R2_seqtab)

getN <- function(x) sum(getUniques(x))

sapply(all_mergers, getN)
sapply(all_dadaFs, getN) #only R1
sapply(all_dadaRs, getN) #only R2

dada2:::is.sequence.table(mergers_seqtab)

colnames(mergers_seqtab)

dim(mergers_seqtab)

rownames(mergers_seqtab) 
colnames(mergers_seqtab) %>% length()
colnames(mergers_seqtab) %>% unique() %>% length()

dim(mergers_seqtab)
str(mergers_seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(mergers_seqtab)))
table(nchar(getSequences(mergers_seqtab))) %>% plot() #size distribution of the ASVs

```

### Remove *chimeras*

*Chimeras* are artificial read pairs that might have been generated erroneously on sequencing. The **DADA2** package estimates the probability of a sequence to be chimeric given the abundancy of its parental sequnces. After chimeric sequences removal, the remaining ASVs length distribution is assessed. On further steps it will be used to restrict analisys to ASVs compatible with each primer amplicons' length interval, in order to keep of unexpected ASVs.

```{r, eval=FALSE}
# 16 - remove chimeras ----

# Merged pairs
mergers_seqtab.nochim <- removeBimeraDenovo(mergers_seqtab, method = "consensus", multithread=TRUE, verbose=TRUE)  

# R1
R1_seqtab.nochim <- R1_seqtab
R1_seqtab.nochim <- removeBimeraDenovo(R1_seqtab, method = "consensus", multithread=TRUE, verbose=TRUE)

# R2
R2_seqtab.nochim <- R2_seqtab
R2_seqtab.nochim <- removeBimeraDenovo(R2_seqtab, method = "consensus", multithread=TRUE, verbose=TRUE)

dim(mergers_seqtab.nochim)
sum(mergers_seqtab.nochim) / sum(mergers_seqtab) 

## =   0.9951358 , perda de 0,48% na abundancia -> estes são quimeras

#count proportion of ASVs of a given length
table(nchar(getSequences(mergers_seqtab.nochim)))
table(nchar(getSequences(mergers_seqtab.nochim))) %>% plot()

View(mergers_seqtab.nochim)
dim(mergers_seqtab.nochim)

```

### Count reads and remaining ASVs

```{r, eval=FALSE}
# 17 - count reads proportion throughout the pipeline ----

# Preparing subtables with named rows to combine latter raw files

getN <- function(x) sum(getUniques(x))

raw_reads <- sample_idx_tbl %>%
                        mutate("Unique_File_name_Primer" = Unique_File_name) %>% 
                        select(-c("Primer","Unique_File_name_Primer")) %>% 
                        filter(Stage %in% c("FWD_R1","FWD_R2")) %>% 
                        unique()

raw_reads$`Read file` %>% unique()

raw_reads %>% rownames()

raw_reads_counts <- tibble()

raw_reads$`Read file`[file.exists(raw_reads$`Read file`)] %>% length()

raw_reads_counts <- ShortRead::countFastq(dirPath = raw_reads$`Read file`[file.exists(raw_reads$`Read file`)]) %>%
  as_tibble(rownames = "Read file") %>%
  left_join(y = (raw_reads %>%  mutate(`Read file` = basename(`Read file`))),
            by = "Read file") %>%
  select(c( "Unique_File_name","records","Stage")) %>%
  unique() %>%
  pivot_wider(names_from = "Stage",
              values_from = "records") %>%
  dplyr::rename(
  "Raw reads (R1)" = "FWD_R1",
  "Raw reads (R2)" = "FWD_R2")
                    
raw_reads_counts <- raw_reads_counts %>%
  mutate("Unique_File_name_Primer" = Unique_File_name) %>%
  select(-c("Unique_File_name"))

# Verificando quantas amostras sao
rownames(raw_reads_counts) # 10, todas amostras estao aqui

View(raw_reads_counts)

# Paired files ----

all_filtered_out

# Cutadapt files ----

# View(sample_idx_tbl)
View(cut_files_per_primer)

sample_idx_tbl$Stage %>% unique()

# cut_reads <- sample_idx_tbl %>%
cut_reads <- cut_files_per_primer %>%
  filter(Stage %in% c("FWD_R1_cutadapt","FWD_R2_cutadapt")) %>%
  unique() %>%
  mutate("Unique_File_name_Primer" = Unique_File_name)

cut_reads$`Read file` %>% unique()
          
cut_reads %>% rownames()
          
cut_reads$`Read file`[file.exists(cut_reads$`Read file`)] %>% length()
          
cut_reads_counts <- ShortRead::countFastq(dirPath = cut_reads$`Read file`[file.exists(cut_reads$`Read file`)]) %>%
  as_tibble(rownames = "Read file") %>%
  left_join(y = (cut_reads %>%  mutate(`Read file` = basename(`Read file`))),
            by = "Read file") %>%
  select(c( "Unique_File_name","Unique_File_name_Primer","records","Stage")) %>%
  unique() %>%
  pivot_wider(names_from = "Stage",
              values_from = "records")

cut_reads_counts %>%
  group_by(Unique_File_name) %>%
  summarise("FWD_R1_cutadapt" =  sum(FWD_R1_cutadapt),
            "FWD_R2_cutadapt" =  sum(FWD_R2_cutadapt)) %>% 
  View()

# Repaired ----

View(all_repaired)

all_filtered_out_and_raw <- all_repaired %>%
  left_join(y = all_filtered_out,
            by = "Unique_File_name") %>%
  dplyr::rename(
    # "Raw reads (R1)" = "FWD_R1",
    # "Raw reads (R2)" = "FWD_R2",
    # "Primer detected reads (R1)" = "FWD_R1_cutadapt",
    # "Primer detected reads (R2)" = "FWD_R2_cutadapt",
    # "Primer detected reads (pairs)"= "Raw reads (FWD pairs)",
    # "Paired reads (pairs)"= "Paired reads (FWD pairs)"
    ) %>%
  relocate("Unique_File_name_Primer","Unique_File_name",
           "Raw reads (pairs)" ,"Paired reads", "Primer detected reads", "Repaired reads") %>%
  select(!starts_with(match = "Prop"))

View(all_filtered_out_and_raw)

# Denoised ----

# Primer removed (for non demux only)

tbl_Denoised_R1 <- (sapply(all_dadaFs, getN) %>% 
                      as_tibble(rownames = "Unique_File_name_Primer")) %>% 
  `colnames<-`(c("Unique_File_name_Primer", "Denoised (R1)")) 

tbl_Denoised_R2 <- (sapply(all_dadaRs, getN) %>% 
                      as_tibble(rownames = "Unique_File_name_Primer")) %>% 
  `colnames<-`(c("Unique_File_name_Primer", "Denoised (R2)")) 

# Merged  ----

tbl_Merged <- (rowSums(mergers_seqtab) %>% 
                 as_tibble(rownames = "Unique_File_name_Primer")) %>% 
  `colnames<-`(c("Unique_File_name_Primer", "Merged"))

#non-chimeric ----
tbl_Non_chimeric_merged <- (rowSums(mergers_seqtab.nochim) %>% as_tibble(rownames = "Unique_File_name_Primer")) %>% 
  `colnames<-`(c("Unique_File_name_Primer", "Non-chimeric Merged"))

# Combine all counts by sample to plot ----

all_track <- all_filtered_out_and_raw %>%
  dplyr::select(-c(starts_with("Prop"))) %>%
  left_join(tbl_Denoised_R1,
            by = "Unique_File_name_Primer") %>%
  left_join(tbl_Denoised_R2,
            by = "Unique_File_name_Primer") %>%
  left_join(tbl_Merged,by = "Unique_File_name_Primer") %>%
  left_join(tbl_Non_chimeric_merged,
            by = "Unique_File_name_Primer") %>%
  left_join(unique(cut_files_per_primer[,c("Primer","Unique_File_name_Primer","Project",
                                           # "Researcher",
                                           "Type","Sample")]),
            by = "Unique_File_name_Primer") %>% 
  relocate(c("Project",
             "Primer",
             "Sample",
             "Unique_File_name",
             "Type"
             # "Unique_File_name_Primer"
             )) %>% 
  select(-c("Unique_File_name_Primer"))

View(all_track)

all_track %>% colnames()

View(all_track)

# Save counts table
writexl::write_xlsx(x = all_track,
                    path = paste0(results_path,"/",prjct_rad,"-reads_and_seqs_counts-",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)

colnames(all_track) %>% paste0(collapse = '",\n"') %>% cat()

# transform tibble to long format, better for ggplot
  track_tbl <- bind_rows(all_track) %>%
    # dplyr::select(-c("Total usable merged seqs/Raw reqs(%)")) %>% 
  gather(key = "Stage", 
        value = "Read counts",
        "Raw reads (pairs)",
        # "Raw reads (R1)",
        # "Raw reads (R2)",
        # "Primer detected reads (R1)",
        # "Primer detected reads (R2)",
        "Paired reads", 
        "Primer detected reads",
        "Repaired reads", 
        "Denoised (R1)",
        "Denoised (R2)",
        "Merged",
        "Non-chimeric Merged"
        # ,
        # "Concatenated",
        # "Non-chimeric Concatenated",
        # "R1 + R2",
        # "Non-chimeric R1 + R2"
        ) %>%
  mutate(Stage = factor(Stage, levels = c(
        "Raw reads (pairs)",
        # "Raw reads (R1)",
        # "Raw reads (R2)",
        # "Primer detected reads (R1)",
        # "Primer detected reads (R2)",
        "Paired reads", 
        "Primer detected reads",
        "Repaired reads", 
        "Denoised (R1)",
        "Denoised (R2)",
        "Merged",
        "Non-chimeric Merged"
        # ,
        # "Concatenated",
        # "Non-chimeric Concatenated",
        # "R1 + R2"
        # "Non-chimeric R1 + R2"
  ))) 
    
    options(scipen = 22)
  
# plot samples readssequences along the pipeline ---
    
scales::show_col(viridis::viridis(n = 15))
viridis10 <- (viridis::turbo(n = 17))
scales::show_col(base::rev(viridis10[c(1,1,2,5,5,7,8,10,11,13,14,15,15,17,17)]))

#multiple plots ---
for (project in all_projects) {

project_name <- project %>% str_replace_all(pattern = " ",
                                        replacement = "_")
project_name

N_samples <- track_tbl %>%
  filter(str_detect(string = Project, pattern = project)) %>% 
  pull(Unique_File_name) %>% unique() %>% length()

track_plot <- track_tbl %>% 
filter(str_detect(string = Project, pattern = project)) %>% 
  # mutate(Unique_File_name = factor(Unique_File_name, levels =  sample_levels)) %>%
  arrange(Unique_File_name) %>%
  ggplot(aes(y = Stage,
             x = `Read counts`, 
             fill = Stage,
             group = Unique_File_name)) +
             # group = Sample)) +
  geom_bar(stat="identity") + 
  geom_hline(yintercept = 300000, col = 1, linetype = 2) +
  scale_fill_manual(values = alpha(colour = rev(viridis10[c(1,1,2,5,5,7,8,10,11,13,14,15,17,17)]),
  # scale_fill_manual(values = alpha(colour = rev(viridis10[c(1,2,5,7,8,11,13)]),
                                   alpha =  0.75)) +
  # labs(title = paste0(prjct_rad,"-",unique(smp_abd_ID_Final$Project)),
  # labs(title = paste0("EcoMol - ",prjct_rad, " - ", project),
  labs(title = paste0("GMB - ",project),
       subtitle = paste0("Number of sequences per library and Data cleaning/processing stage"
                         # ,
                         # " - Number of samples: ",
                         # length(unique(track_tbl$Unique_File_name)
                         #        )
                         ),
       x = "Number of sequences",
       y = "Data processing stage") +
  # geom_label() +
  # facet_wrap(Primer~Sample, ncol = 8) +
  facet_wrap(Unique_File_name~Primer, ncol = 8) +
  # coord_fixed(ratio = 2000) +0
  coord_fixed(ratio = max(track_tbl$`Read counts`,na.rm = T)/length(unique(track_tbl$Stage))*0.75) +
  theme_bw(base_size = 8) +
  theme(axis.text.x = element_text(angle = 90,hjust = 0.0001,
                                   vjust = -0.00000000001,face = "bold")) +
  theme(legend.position = "bottom") +
  theme(axis.title = ggtext::element_markdown())

track_plot

# save plot
ggsave(file = paste0(figs_path,"/",project_name,"-samples_track.pdf",collapse = ""),
# ggsave(file = paste0(results_path,"/",
#                      unique(smp_abd_ID_Final$Project),"/",
#                      unique(smp_abd_ID_Final$Project),"-samples_track.pdf",collapse = ""),
plot = track_plot,
device = "pdf",
width = ifelse(N_samples <= 10, 8, 12),
height = ifelse(N_samples <= 10, 4, N_samples/6),
units = "cm", 
scale = 5, 
limitsize = F,
dpi = 600)

}
```

## Classify taxonomy

On this step the ASVs identified by the **DADA2** pipeline, jointly for all libraries of each primer, are associated (or not) to any of the sequences on the Reference 12S Sequences Database. DADA2 has two strategies to identify taxa. The first, *assignSpecies*, identify perfect matches of the ASVs in the Reference Database. The second, *assignTaxonomy*, use a RDP Naive Bayesian Classifier algorithm (Wang, 2007) with kmer size 8 and 100 bootstrap replicates to associate ASVs to the Reference Database Sequences. In the latter, the taxonomy ranks classification is proportional to the sequence similarity, although this relation is not yet clear to us.

### Exact species

```{r, eval=FALSE}

#19 - classify taxonomy exactly ----

str(mergers_seqtab)

rownames(mergers_seqtab.nochim)

dim(mergers_seqtab)

# Merged pairs
mergers_sps <- dada2::assignSpecies(seqs = mergers_seqtab.nochim, allowMultiple = 10,
                                    refFasta =  "/home/heron/prjcts/eDNA_fish/DB/jun23/DB/LGC12Sdb-jun23-dada_SP_fullDB.fasta",
                                # refFasta =  "/home/heron/prjcts/fish_eDNA/data/refs/db/BOLD/BOLD_dada_tax_Sp.fasta",
                                tryRC=TRUE,
                                n = 20000,
                                verbose = TRUE)
# R1 reads
R1_sps <- dada2::assignSpecies(seqs = R1_seqtab.nochim, allowMultiple = 10,
                               refFasta =  "/home/heron/prjcts/eDNA_fish/DB/jun23/DB/LGC12Sdb-jun23-dada_SP_fullDB.fasta",
                               # refFasta =  "/home/heron/prjcts/eDNA_fish/data/refs/db/BOLD/BOLD_dada_tax_Sp.fasta",
                               tryRC=TRUE,
                               n = 20000,
                               verbose = TRUE)
#R2 reads
R2_sps <- dada2::assignSpecies(seqs = R2_seqtab.nochim,allowMultiple = 10,
                               refFasta =  "/home/heron/prjcts/eDNA_fish/DB/jun23/DB/LGC12Sdb-jun23-dada_SP_fullDB.fasta",
                               # refFasta =  "/home/heron/prjcts/eDNA_fish/data/refs/db/BOLD/BOLD_dada_tax_Sp.fasta",
                               tryRC=TRUE,
                               n = 20000,
                               verbose = TRUE)
     
```

### Unexact species or other taxonomic ranks

```{r, eval=FALSE}
#20 - classify taxonomy ----
sample_names(all_taxa)

mergers_taxa <- dada2::assignTaxonomy(seqs = mergers_seqtab.nochim,
                                            refFasta =  "/home/heron/prjcts/eDNA_fish/DB/jun23/DB/LGC12Sdb-jun23-dada_tax_fullDB.fasta",
                                            multithread=TRUE, tryRC=TRUE,
                                            taxLevels = c("Kingdom","Phylum","Class","Order","Family",
                                                          "Genus", "Species","Specimen","Basin"),
                                            outputBootstraps = TRUE, verbose = TRUE )
      
R1_taxa <- dada2::assignTaxonomy(seqs = R1_seqtab.nochim,
                                 refFasta =  "/home/heron/prjcts/eDNA_fish/DB/jun23/DB/LGC12Sdb-jun23-dada_tax_fullDB.fasta",
                                 multithread=TRUE, tryRC=TRUE,
                                 taxLevels = c("Kingdom","Phylum","Class","Order","Family",
                                               "Genus", "Species","Specimen","Basin"),
                                 outputBootstraps = TRUE, verbose = TRUE )

R2_taxa <- dada2::assignTaxonomy(seqs = R2_seqtab.nochim,
                                 refFasta =  "/home/heron/prjcts/eDNA_fish/DB/jun23/DB/LGC12Sdb-jun23-dada_tax_fullDB.fasta",
                                 multithread=TRUE, tryRC=TRUE,
                                 taxLevels = c("Kingdom","Phylum","Class","Order","Family",
                                               "Genus", "Species","Specimen","Basin"),
                                 outputBootstraps = TRUE, verbose = TRUE )

# Merged pairs ----
#convert dada2 exact species object to tibble
mergers_csv_sp <- mergers_sps %>% 
  as_tibble() %>% 
  mutate(ASV = rownames(mergers_sps)) %>% 
  dplyr::rename("Exact Genus (DADA2)" = "Genus",
          "exact Species (DADA2)" = "Species")

#convert dada2 taxonomy object to tibble
mergers_csv_taxa <- mergers_taxa$tax %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2)")) %>% 
  mutate(ASV = rownames(mergers_taxa$tax))

#adding bootstrap & exact species
mergers_csv_taxa_boot <- mergers_taxa$boot %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2 bootstrap)")) %>% 
  mutate(ASV = rownames(mergers_taxa$boot))

# R1 reads ----
#convert dada2 exact species object to tibble
R1_csv_sp <- R1_sps %>% 
  as_tibble() %>% 
  mutate(ASV = rownames(R1_sps)) %>% 
  dplyr::rename("Exact Genus (DADA2)" = "Genus",
          "exact Species (DADA2)" = "Species")
  
#convert dada2 taxonomy object to tibble
R1_csv_taxa <- R1_taxa$tax %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2)")) %>% 
  mutate(ASV = rownames(R1_taxa$tax))

#adding bootstrap & exact species
R1_csv_taxa_boot <- R1_taxa$boot %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2 bootstrap)")) %>% 
  mutate(ASV = rownames(R1_taxa$boot))

# R2 reads ----      
#convert dada2 exact species object to tibble
R2_csv_sp <- R2_sps %>% 
  as_tibble() %>% 
  mutate(ASV = rownames(R2_sps)) %>% 
  dplyr::rename("Exact Genus (DADA2)" = "Genus",
          "exact Species (DADA2)" = "Species")
  
#convert dada2 taxonomy object to tibble
R2_csv_taxa <- R2_taxa$tax %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2)")) %>% 
  mutate(ASV = rownames(R2_taxa$tax))

#adding bootstrap & exact species
R2_csv_taxa_boot <- R2_taxa$boot %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2 bootstrap)")) %>% 
  mutate(ASV = rownames(R2_taxa$boot))

# combine all
mergers_csv_IDs <- mergers_csv_taxa %>% 
  left_join(mergers_csv_taxa_boot,
            by = "ASV") %>% 
  left_join(mergers_csv_sp,
            by = "ASV") %>% 
  dplyr::select(c("ASV",
           starts_with("King"),
           starts_with("Phy"),
           starts_with("Class"),
           starts_with("Ord"),
           starts_with("Fam"),
           starts_with("Gen"),
           starts_with("Species"),
           starts_with("Specimen"),
           starts_with("Basin"),
           starts_with("Exac"),
           )) %>% 
   mutate(`Exact GenSp (DADA2)` = paste(`Exact Genus (DADA2)`,`exact Species (DADA2)`,sep=" "))
  
# combine all
R1_csv_IDs <- R1_csv_taxa %>% 
  left_join(R1_csv_taxa_boot,
            by = "ASV") %>% 
  left_join(R1_csv_sp,
            by = "ASV") %>% 
  dplyr::select(c("ASV",
           starts_with("King"),
           starts_with("Phy"),
           starts_with("Class"),
           starts_with("Ord"),
           starts_with("Fam"),
           starts_with("Gen"),
           starts_with("Species"),
           starts_with("Specimen"),
           starts_with("Basin"),
           starts_with("Exac"),
           )) %>% 
   mutate(`Exact GenSp (DADA2)` = paste(`Exact Genus (DADA2)`,`exact Species (DADA2)`,sep=" "))
  
# combine all
R2_csv_IDs <- R2_csv_taxa %>% 
  left_join(R2_csv_taxa_boot,
            by = "ASV") %>% 
  left_join(R2_csv_sp,
            by = "ASV") %>% 
  dplyr::select(c("ASV",
           starts_with("King"),
           starts_with("Phy"),
           starts_with("Class"),
           starts_with("Ord"),
           starts_with("Fam"),
           starts_with("Gen"),
           starts_with("Species"),
           starts_with("Specimen"),
           starts_with("Basin"),
           starts_with("Exac"),
           )) %>% 
   mutate(`Exact GenSp (DADA2)` = paste(`Exact Genus (DADA2)`,`exact Species (DADA2)`,sep=" "))

# Save environment
base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_DADA2_assign.RData"))
```

Here the **DADA2** pipeline ends.

## Phyloseq

On this step the ASVs associated to taxonomic ranks by **DADA2** and their respective counts by library, are combined using the **Phyloseq** package.

### Generate sample metadata table

Here the experiment metadata is associated to each sample.

```{r, eval=FALSE}
# 22 - create sample table ----

primers_n_samples %>% colnames()

all_samdf <- primers_n_samples %>% 
  select(c("Project",
           "Sample",
           # "Researcher",
           # "Analysis",
           "Unique_File_name",
           "Primer",
           "Lib",
           "Type",
           # starts_with("Metadata"),
           # "obs",
           # "Extraction control",
           # "Filtration control",
           "PCR control"
           )) %>%  
  unique() %>%
  # separate_longer_delim(cols = c("Primer"),delim = ";") %>%                      # separando por primer
  tidyr::separate_longer_delim(cols = Primer,delim = ";") %>% 
  unite(col = "Unique_File_name_Primer",
        Primer,Unique_File_name,
        sep = "--",
        remove = F) %>%
  as.data.frame()

samdf <- all_samdf

rownames(samdf) <- samdf$Unique_File_name_Primer
```

This sample metadata table was created with the information available for the samples analyzed on this first run. This table must be customized for each experiment.

### **Phyloseq** data interpretation

```{r, eval=FALSE}
#23 - interpret dada on phyloseq ----
rownames(samdf)
str(samdf)

dim(mergers_seqtab)
dim(mergers_seqtab.nochim)

mergers_ps <- phyloseq::phyloseq(phyloseq::otu_table(mergers_seqtab.nochim, taxa_are_rows = FALSE),
                                 phyloseq::sample_data(samdf),
                                 phyloseq::tax_table(mergers_taxa$tax))

R1_ps <- phyloseq::phyloseq(phyloseq::otu_table(R1_seqtab.nochim, taxa_are_rows = FALSE),
                            phyloseq::sample_data(samdf),
                            phyloseq::tax_table(R1_taxa$tax))

R2_ps <- phyloseq::phyloseq(phyloseq::otu_table(R2_seqtab.nochim, taxa_are_rows = FALSE),
                            phyloseq::sample_data(samdf),
                            phyloseq::tax_table(R2_taxa$tax))
```

### Merge and Flex Phyloseq results

Many different graphics can be generated, together or in isolation, for all primers/libraries and taxonomic ranks.

```{r, eval=FALSE}
#24 - merge ps analysis ----
# combine all pyloseq objects in one
# by doing so, all ASVs will be combined and some will have 0 abundance

mergers_ps_tbl <- phyloseq::psmelt(mergers_ps) %>% 
  as_tibble() %>% 
  mutate(`Read origin` = "merged") %>% 
  filter(Abundance >= 1) %>% 
  mutate("ASV tip" = OTU) %>% 
  dplyr::rename("ASV" = "OTU")

R1_ps_tbl <- phyloseq::psmelt(R1_ps) %>% 
  as_tibble() %>% 
  mutate(`Read origin` = "R1") %>%  
  filter(Abundance >=1) %>% 
  mutate("ASV tip" = OTU) %>% 
  dplyr::rename("ASV" = "OTU")

R2_ps_tbl <- phyloseq::psmelt(R2_ps) %>% 
  as_tibble() %>% 
  mutate(`Read origin` = "R2") %>%  
  filter(Abundance >=1) %>% 
  mutate("ASV tip" = OTU) %>% 
  dplyr::rename("ASV" = "OTU")

#combine ps tables from all ASVs inputs
all_ps_tbl <- bind_rows(
  mergers_ps_tbl,
  # R1_ps_tbl,  
  # R2_ps_tbl
  ) %>%
  select(-c("Sample")) %>%  
  dplyr::rename("Sample" = "sample_Sample") %>%
  separate(col = Unique_File_name_Primer,
           into = c("Primer","Unique_File_name"),
           sep = "--",remove = F) 

View(all_ps_tbl)

all_ps_tbl$ASV %>% unique()
#clear zero abundance rows

mergers_ps_tbl$Sample %>%  unique()

mergers_ps_tbl$Primer %>%  unique()

colnames(mergers_ps_tbl)
```

### Calculate sample abundances

```{r, eval=FALSE, echo=TRUE}
{
  all_ps_tbl <- all_ps_tbl %>%
  mutate("Relative abundance to all samples" = 0,
         "Relative abundance on sample" = 0,
         "Sample total abundance" = 0)
  
  abd_total <- sum(all_ps_tbl$Abundance)
  
  all_ps_tbl <- all_ps_tbl %>%
    dplyr::group_by(Unique_File_name,`Read origin`) %>%        #now the abundance on sample is for merged/R1/R2 separetely
    dplyr::mutate("Sample total abundance" = sum(Abundance)) %>% 
    dplyr::mutate("Relative abundance to all samples" = round((Abundance/abd_total), digits = 6)*100) %>% 
    dplyr::mutate("Relative abundance on sample" =  round((Abundance/`Sample total abundance`), digits = 6)*100) %>%
    dplyr::relocate(`Sample total abundance`,`Relative abundance to all samples`,`Relative abundance on sample`) %>% 
    dplyr::ungroup()
}

View(all_ps_tbl)
```

### Check ASVs legths pre-BLAST

```{r, eval=FALSE}

all_ps_tbl <- all_ps_tbl %>%
  mutate("ASV Size (pb)" = nchar(ASV))

# Tamanho das ASVs por amostra e Read origin ---- 
scales::show_col(viridis::viridis(n=10))
scales::show_col(viridis::turbo(n=12))

ASV_length_by_Sample <- all_ps_tbl %>%
  # mutate(Sample=factor(Sample,levels = sample_levels)) %>% 
  ggplot(aes(y=Unique_File_name,
  # ggplot(aes(y= interaction(`Read origin`,Unique_File_name_Primer),
             x=`ASV Size (pb)`,
             fill = `Read origin`,
             # fill = Primer,
             col = `Read origin`,
             # col = Primer,
             size =`Relative abundance on sample`,
             alpha = 0.25
             )) +
  geom_jitter(height = 0.3,
              width = 0.3) +
  scale_x_continuous(breaks = c(seq(20,
                                    max(all_ps_tbl$`ASV Size (pb)`),10)),
  # scale_x_continuous(breaks = c(seq(20,280,10)),
                     expand = c(0.02,0.02)) +
  scale_shape_manual(name = "Identification\n     satatus",
                                             values = c(21,4),
                                             labels=c("BLAST IDed","no ID")) +
  scale_fill_manual(values = c(viridis::turbo(n=4))) +
  scale_colour_manual(values = c(viridis::turbo(n=4))) +
  xlab("Tamanho da ASV (pb)") +
  ylab("Amostra") +
  ggtitle(label = paste0("GMB", " ", project_name),
          subtitle = "Distribution of ASVs size and Read origin per Sample considering all identified ASVs") +
  theme_bw(base_size = 8) +
  theme(legend.position = "right")+
  geom_vline(xintercept = c(10,max(all_ps_tbl$`ASV Size (pb)`))) +
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +
# +
#   guides(alpha="none",
#          color="none") 
# +
  facet_grid(rows = vars(Type, 
                         # Sample, 
                         Primer),scales ='free_y', space ='free_y') +
  # facet_grid(rows = vars(Sample,`Read origin`),scales ='free_y', space ='free_y') +
  theme(strip.text.y = element_text(size = 10,angle = 0))
  
ASV_length_by_Sample

ggsave(file = paste0(figs_path,"/",prjct_rad,"-ASV_length_by_sample-ALL-ASVs-IDs.pdf",collapse = ""),
     plot = ASV_length_by_Sample,
     device = "pdf",
     width = 25,
     height = 30,
     units = "cm",
     dpi = 600)
```

## BLASTn identification

```{r, eval=FALSE}
# blastn ----
## Annotate all ASVs by blastN
### select ASVs for BLASTn search ----

all_ps_tbl$ASV %>% unique()
all_ps_tbl$`ASV tip` %>% unique()

asvs_blast_all <- all_ps_tbl %>%
  pull(ASV) %>%
  as.character() %>% 
  unique()

length(asvs_blast_all)
asvs_blast_all %>% nchar() %>% table() %>% plot()

remotes::install_github("heronoh/BLASTr", force = TRUE)
devtools::reload(pkg = "/home/heron/R/x86_64-pc-linux-gnu-library/4.1/BLASTr",quiet = F)

# Parte 1 ----
{
  tictoc::tic()
  
  blast_res_1 <- BLASTr::parallel_blast(
    # db_path = "/data/databases/nt_2024/nt", ## Para nao peixes
    db_path = '"/data/databases/nt_2024/nt /home/gabriel/projetos/peixes-eDNA/databases/LGC12Sdb/files/LGC12Sdb_2024-05-11_fh.fasta"', ## Para peixes
    # asvs = asvs_blast_all[1:10],
    asvs = asvs_blast_all,
    out_file = paste0(blast_path,"/","blast_out_res_1.csv"),
    out_RDS = paste0(blast_path,"/","blast_out_res_1.RDS"),
    total_cores = 70,
    perc_id = 80,
    num_threads = 2,
    perc_qcov_hsp = 80,
    num_alignments = 3,
    blast_type = "blastn"
    )
  
  blast_res_backup1 <- blast_res_1
  tictoc::toc()
  Sys.time()

# Save environment
base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_BLAST_1.RData"))
}

# Parte 2 ----
{
  # tictoc::tic()
  # 
  # blast_res_2 <- BLASTr::parallel_blast(
  #   db_path = "/data/databases/nt_jun2023/nt",
  #   asvs = asvs_blast_all[409:816],
  #   out_file = paste0(blast_path,"/","blast_out_res_2.csv"),
  #   out_RDS = paste0(blast_path,"/","blast_out_res_2.RDS"),
  #   total_cores = 70,
  #   perc_id = 80,
  #   num_threads = 2,
  #   perc_qcov_hsp = 80,
  #   num_alignments = 3,
  #   blast_type = "blastn"
  #   )
  # blast_res_backup2 <- blast_res_2
  # tictoc::toc()
  # Sys.time()
  
# Save environment
# base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_BLAST_2.RData"))
}

blast_res <- bind_rows(
  blast_res_1
  # ,
  # blast_res_2
  ) %>% 
  # select(-c("OTU")) %>%
  filter(!is.na(`1_subject header`))

blast_res_bckp <- blast_res

View(blast_res)

# Save environment
base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_post-BLAST.RData"))
```

### Prepare BLASTr output for taxonomic retrieval

```{r,echo=TRUE, eval=FALSE}

# Recuperar a taxonomia das identificações do NT e 12S ----

## Editando o output do BLASTr 

# Editando os hits obtidos com o 12sLGCdb para ficarem semelhantes aos hits obtidos com o NT

blast_res_f <- blast_res %>% 
  mutate("1_DB" = ifelse(is.na(`1_subject`), NA, ifelse(grepl(".\\..", `1_subject`), "NT", "12sLGCdb")),
         "2_DB" = ifelse(is.na(`2_subject`), NA, ifelse(grepl(".\\..", `2_subject`), "NT", "12sLGCdb")),
         "3_DB" = ifelse(is.na(`3_subject`), NA, ifelse(grepl(".\\..", `3_subject`), "NT", "12sLGCdb")))

View(blast_res_f)

# Editando o conteudo das colunas _subject header e _subject 

update_subject_header <- function(df) {
  for (i in 1:3) {
    header_col <- paste0(i, "_subject header")
    subject_col <- paste0(i, "_subject")
    
    df <- df %>%
      rowwise() %>%
      mutate(
        # Atualiza 'subject header' se estiver vazio ou contiver apenas '\n'
        !!header_col := ifelse(
          is.na(get(header_col)) | str_trim(get(header_col)) == "" | str_trim(get(header_col)) == "\n",
          str_remove(get(subject_col), "^[^-]+-") %>%
            str_replace_all("_", " "),
          get(header_col)
        ),
        # Corrige casos onde 'subject header' é um nome isolado
        !!header_col := ifelse(
          str_detect(get(header_col), "^[a-zA-Z]+$"),
          str_extract(get(subject_col), "(?<=-)[^-]+") %>%
            str_replace_all("_", " ") %>%
            paste(str_extract(get(header_col), "[^\\.]+$"), collapse = " "),
          get(header_col)
        ),
        # Atualiza 'subject' removendo a parte após o primeiro '-'
        !!subject_col := ifelse(
          str_detect(get(header_col), "[a-zA-Z]+\\s[a-zA-Z]+"),
          str_remove(get(subject_col), "-[^-]+$"),
          get(subject_col)
        )
      ) %>%
      ungroup()
  }
  return(df)
}

# Aplicar a função à tabela
blast_res_f <- update_subject_header(blast_res_f)

## Loop para recuperar o TaxID dos hits 1,2 e 3

# Crie um ambiente para armazenar os pares organism_name e taxid
taxid_cache <- new.env(hash = TRUE, parent = emptyenv())

for (hit_number in 1:3) {
  db_column <- paste0(hit_number, "_DB")
  subject_header_column <- paste0(hit_number, "_subject header")
  staxid_column <- paste0(hit_number, "_staxid")
  
  spp_db <- unique(blast_res_f[[db_column]][blast_res_f[[db_column]] == "12sLGCdb"])
  
  for (db_value in spp_db) {
    filtered_data <- blast_res_f %>%
      filter(.data[[db_column]] == db_value)
    
    for (subject_header in filtered_data[[subject_header_column]]) {
      organism_name <- word(subject_header, 1)
      
      if (!is.na(organism_name)) {
        if (exists(organism_name, where = taxid_cache)) {
          # Se o TaxID está em cache, sinaliza isso com uma mensagem
          cat("TaxID for", organism_name, "found in cache.\n")
          taxid <- get(organism_name, envir = taxid_cache)
        } else {
          # Se não estiver em cache, recupera o TaxID
          taxid <- retrieve_taxid(organism_name)
          
          if (!is.na(taxid)) {
            assign(organism_name, taxid, envir = taxid_cache)
          }
        }
        
        if (!is.na(taxid)) {
          blast_res_f[[staxid_column]][blast_res_f[[db_column]] == db_value & blast_res_f[[subject_header_column]] == subject_header] <- taxid
        } else {
          cat("Failed to retrieve TaxID for:", organism_name, "\n")
        }
      } else {
        cat("Organism name is NA. Skipped.\n")
      }
    }
  }
  
  # ASVs sem TaxID
  no_taxid <- blast_res_f %>%
    filter(.data[[staxid_column]] == "N/A")
  
  for (subject_header in no_taxid[[subject_header_column]]) {
    organism_name <- word(subject_header, 1)
    
    if (!is.na(organism_name)) {
      if (exists(organism_name, where = taxid_cache)) {
        # Se o TaxID está em cache, sinaliza isso com uma mensagem
        cat("TaxID for", organism_name, "found in cache\n")
        taxid <- get(organism_name, envir = taxid_cache)
      } else {
        # Se não estiver em cache, recupera o TaxID
        taxid <- retrieve_taxid(organism_name)
        
        if (!is.na(taxid)) {
          assign(organism_name, taxid, envir = taxid_cache)
        }
      }
      
      if (!is.na(taxid)) {
        blast_res_f[[staxid_column]][blast_res_f[[subject_header_column]] == subject_header] <- taxid
      } else {
        cat("Failed to retrieve TaxID for:", organism_name, "\n")
      }
    } else {
      cat("Organism name is NA.\n")
    }
  }
}

# Verifique o resultado final para saber se existem ASVs sem TaxID

sapply(blast_res_f[, c("1_staxid", "2_staxid", "3_staxid")], function(x) {
  any(str_detect(x, "\\bN/A\\b"))}) # Se tudo der False, podemos prosseguir

blast_res_full <- blast_res_f

colnames(blast_res_full)

blast_res_full <- blast_res_full %>% 
  select(-c("OTU")) %>%
  filter(!is.na("1_subject header"))

colnames(blast_res_full)

```


### Prepare BLASTn identifications for taxonomic retrieval

```{r,echo=TRUE, eval=FALSE}

# overview the identifications ----
blast_res_full$`1_subject header` %>% unique() %>% sort() %>% 
  tibble() %>% View()

#set hits with poor names to remove from results
bad_1res_IDs <- c(
  "Uncultured",
  "uncultured",
  "Uncultured organism clone",
  "Uncultured prokaryote",
  "Eukaryotic synthetic construct",
  "16S rRNA amplicon fragment",
  "Uncultured Candidatus",
  "Uncultured bacterium",
  "Uncultured archaeon clone",
  "Complete Metagenome-Assembled",
  "PREDICTED: Nomascus",               # esse macaco é gente
  "Invertebrate environmental sample"
  ) %>% 
  paste0(collapse = "|")

# Se necessario
blast_res_full_bckp <- blast_res_full
# blast_res_full <- blast_res_full_bckp

blast_res_full <- blast_res_full %>%
  mutate("blast ID" = "blast ID",
         "blast ID Origin" = "blast ID Origin",
         "query_taxID" = "query_taxID")

# pick BLASTn res IDs and mark result origin ----
for (asv in 1:nrow(blast_res_full)) {
  
  if (stringr::str_detect(string = blast_res_full$`1_subject header`[asv], pattern = bad_1res_IDs) & 
      !is.na(blast_res_full$`2_subject header`[asv])) {
    
    blast_res_full$`blast ID`[asv] <- substr(as.character(blast_res_full$`2_subject header`[asv]), 1, 40)
    blast_res_full$`blast ID Origin`[asv] <- "2_"
    blast_res_full$query_taxID[asv] <- blast_res_full$`2_staxid`[asv]
    
    if (stringr::str_detect(string = blast_res_full$`2_subject header`[asv], pattern = bad_1res_IDs) & 
        !is.na(blast_res_full$`3_subject header`[asv])) {
      
      blast_res_full$`blast ID`[asv] <- substr(as.character(blast_res_full$`3_subject header`[asv]), 1, 40)
      blast_res_full$`blast ID Origin`[asv] <- "3_"
      blast_res_full$query_taxID[asv] <- blast_res_full$`3_staxid`[asv]
      
      if (stringr::str_detect(string = blast_res_full$`3_subject header`[asv], pattern = bad_1res_IDs)) {
        
        blast_res_full$`blast ID`[asv] <- "Match_not_reliable"
        blast_res_full$`blast ID Origin`[asv] <- NA
        blast_res_full$query_taxID[asv] <- NA
      }
      ## Adicao que eu fiz para poder corrigir o resultado para Match_not_reliable nesta situacao:
    } else if (stringr::str_detect(string = blast_res_full$`2_subject header`[asv], pattern = bad_1res_IDs) & 
               is.na(blast_res_full$`3_subject header`[asv])) {
      
      blast_res_full$`blast ID`[asv] <- "Match_not_reliable"
      blast_res_full$`blast ID Origin`[asv] <- NA
      blast_res_full$query_taxID[asv] <- NA
    }
    
  } else {
    if (stringr::str_detect(string = blast_res_full$`1_subject header`[asv], pattern = bad_1res_IDs) & 
        is.na(blast_res_full$`2_subject header`[asv])) {
      
      blast_res_full$`blast ID`[asv] <- "Match_not_reliable"
      blast_res_full$`blast ID Origin`[asv] <- NA
      blast_res_full$query_taxID[asv] <- NA
      
    } else {
      blast_res_full$`blast ID`[asv] <- substr(as.character(blast_res_full$`1_subject header`[asv]), 1, 40)
      blast_res_full$`blast ID Origin`[asv] <- "1_"
      blast_res_full$query_taxID[asv] <- blast_res_full$`1_staxid`[asv]
    }
  }
}

View(blast_res_full)

blast_res_full$`blast ID` %>% unique() %>% sort()
blast_res_full$`blast ID Origin` %>% table() 

blast_res_full$`blast ID` <- blast_res_full$`blast ID` %>% str_replace(pattern = ": ", replacement = ":")

blast_res_full$`blast ID`[blast_res_full$`blast ID` %>%
                            stringr::str_detect(pattern = "^  |^ |Uncultured |uncultured |Candidatus |MAG:|MAG: |MAG TPA_asm: |TPA_asm: |^Cf. |\n|candidate division |TPA: ")] %>% 
  sort() %>% 
  unique()

# Criei um objeto para salvar os padroes que devem ser removidos. Tirei o cf. para mante-lo nos resultados
to_remove <- "^  |^ |Uncultured |uncultured |Candidatus |MAG:|MAG: |MAG TPA_asm: |TPA_asm: |TPA_asm:|\n|candidate division |TPA: "

blast_res_full$`blast ID`<- blast_res_full$`blast ID` %>% 
  stringr::str_remove(pattern = to_remove) %>% 
  stringr::str_remove_all(pattern = "\\[|\\]") %>% 
  # stringr::str_remove(pattern = "\\:.*.$") %>% 
  # stringr::str_replace(pattern = "cf\\. ",replacement = "") %>%
  stringr::str_replace(pattern = "cf\\. ",replacement = "cf\\.") %>%
  stringr::str_replace(pattern = "aff\\. ",replacement = "aff\\.") %>%
  stringr::str_replace(pattern = "nr\\. ",replacement = "") %>% 
  # stringr::str_replace(pattern = "sp\\. ",replacement = "sp\\.") %>% 
  stringr::str_replace(pattern = "\\,",replacement = "") %>% 
  stringr::str_replace(pattern = "sp\\.",replacement = "sp\\. ") %>% 
  stringr::str_replace(pattern = "C\\.",replacement = "Chlamydomonas ") %>% 
  stringr::str_replace(pattern = "\\[(.*?)\\]",replacement = "\\1")
  
# Avaliando as alteracoes
blast_res_full$`blast ID` %>% unique() %>% sort() %>% 
  tibble() %>% View()

blast_res_full$`blast ID` %>% unique() %>% sort(decreasing = T)

# selecting just the first 2 names of BLAST result
for (row in 1:nrow(blast_res_full)) {
  blast_res_full$`blast ID`[row] <- stringr::str_split_fixed(
    string = blast_res_full$`blast ID`[row], pattern = " ", n = 3)[1:2] %>%
    paste0(collapse = " ")
  }

# Avaliando as alteracoes 
blast_res_full$`blast ID` %>% unique() %>% sort() %>% 
  tibble() %>% View()

# Correcoes nos nomes
blast_res_full$`blast ID`<-  blast_res_full$`blast ID` %>% 
  stringr::str_replace(pattern = "cf\\.",replacement = "cf\\. ") %>%
  stringr::str_replace(pattern = "aff\\.",replacement = "aff\\. ") %>% 
  stringr::str_replace(pattern = "PK-199-7",replacement = "sp.") %>% 
  stringr::str_replace(pattern = "genosp.",replacement = "sp.") %>% 
  stringr::str_replace(pattern = "\\:",replacement = "\\: ")

# Avaliando as alteracoes 
blast_res_full$`blast ID` %>% unique() %>% sort() %>% 
  tibble() %>% View()

# Se tiverem IDs que possuem "PREDICTED:", e´ bom deixar
# para mostrar que o gene foi predito mas nao descrito

# correct confusing labels to unify identities 
blast_res_full$`blast ID`[blast_res_full$`blast ID` %in% c("Human DNA",
                                                           "Eukaryotic synthetic",
                                                           "Human chromosome")] <- "Homo sapiens"

# Avaliando as alteracoes 
blast_res_full$`blast ID` %>% unique() %>% sort() %>% 
  tibble() %>% View()

# Verificando se ha blast ids vazias
blast_res_full %>% filter(`blast ID` %in% c(" ","")) %>% 
  tibble() %>%  View()

#Backup
blast_res_full_bckp2 <- blast_res_full
```

### Retrieve complete taxonomy

```{r,echo=TRUE, eval=FALSE}

# blast_res_full <- blast_res_full_bckp2  

# Criacao da coluna max_tax
blast_res_full <- blast_res_full %>%
  mutate("max_tax" = case_when(str_detect(`blast ID`,pattern = "PREDICTED:") ~ paste0(str_remove(`blast ID`, pattern = "PREDICTED: ")," (PREDICTED)"),
                               TRUE ~  str_remove(`blast ID`,pattern = " .*$"))) %>%
  mutate("max_tax" = str_replace(string = max_tax,
                                 pattern = " .*. ",
                                 replacement = " "))

blast_res_full <- blast_res_full %>%
  relocate(`blast ID`,`blast ID Origin`,`query_taxID`,`max_tax`)

View(blast_res_full)

# Avaliando as alteracoes 
blast_res_full$max_tax %>% unique() %>% sort() %>% 
  tibble() %>% View()

# Function to retrieve tax ranks using organism taxID ----

source("/home/heron/prjcts/ecomol/R/extract_taxonomy_taxID.R")
# source("/home/heron/prjcts/BLASTr/R/get_tax_by_taxID.R")
 
# Testando
# extract_taxonomy_taxID("3483")
get_tax_by_taxID("3483")

# Buscando as classificações

# future::plan(future::multisession(), workers = 78)
# 
# taxIDs2search <- blast_res_full$query_taxID %>% unique() %>% na.omit() %>% as.character()
# 
# taxIDs2search %>% class()
# 
# taxonomy_df <- furrr::future_map_dfr(.x = taxIDs2search,
#                                      .f =  get_tax_by_taxID,
#                                      parse_result = TRUE,
#                                      .options = furrr::furrr_options(seed = TRUE))


future::plan(future::multisession(), workers = 78)

taxIDs2search <- blast_res_full$query_taxID %>% unique() %>% na.omit() %>% as.character()

taxIDs2search %>% class()

taxonomy_df <- furrr::future_map_dfr(.x = taxIDs2search,
                                     .f = extract_taxonomy_taxID,
                                     .options = furrr::furrr_options(seed = TRUE))

# (A) check the ones that have not been retrieved
taxIDs2search[!(taxIDs2search %in% c(unique(taxonomy_df$query_taxID)))]

# (B) search the missing ones
taxonomy_df1 <- furrr::future_map_dfr(.x = taxIDs2search[!(taxIDs2search %in% c(unique(taxonomy_df$query_taxID)))],
                                      .f = extract_taxonomy_taxID,
                                      # .f = get_tax_by_taxID,
                                      .options = furrr::furrr_options(seed = TRUE)) 
# (C) combine the results
taxonomy_df <-  bind_rows(taxonomy_df,taxonomy_df1) %>% unique()
  
# (D) check if is there any other still missing
taxIDs2search[!(taxIDs2search %in% c(unique(taxonomy_df$query_taxID)))]

# (E) repeat A, B, C and D until no one is missing
  
taxonomy_df_bckp <- taxonomy_df

View(taxonomy_df)

taxonomy_df <- taxonomy_df %>% 
  filter(!Rank %in% c("no rank","clade"))

taxonomy_tbl <- taxonomy_df %>% 
  # select(-c("TaxId","Sci_name")) %>%
  dplyr::select(-c("TaxId")) %>%
  # dplyr::select(-c("query_taxID")) %>%
  # rename_with(~ str_to_lower(str_remove_all(., " \\(NCBI\\)")), everything()) %>% 
  unique() %>%
  # dplyr::filter(Rank %in% c("kingdom","phylum","class","order","family")) %>%
  filter(Rank %in% c("superkingdom",
                     "kingdom",
                     "phylum",
                     "subphylum",
                     "class",
                     "subclass",
                     "order",
                     "suborder",
                     "family",
                     "subfamily",
                     "genus")) %>%
  tidyr::pivot_wider(
    id_cols = c(query_taxID, Sci_name),
    names_from = Rank,
    values_from = c(ScientificName)) %>%
    # tidyr::pivot_wider(names_from = Rank,values_from = c(ScientificName,TaxId)) %>%
    # dplyr::select(max_tax,dplyr::starts_with("Scie")) %>%
  relocate("Sci_name",
           "query_taxID",
           "superkingdom",
           "kingdom",
           "phylum",
           "subphylum",
           "class",
           "subclass",
           "order",
           "suborder",
           "family",
           "subfamily",
           "genus")

View(taxonomy_tbl)

saveRDS(object = taxonomy_tbl,
        file = paste0(results_path,"/taxonomy_df_from_taxIDs.rds"))

# complete taxonomy tbl missing ranks

# taxonomy_tbl_bckp <- taxonomy_tbl
# taxonomy_tbl <- taxonomy_tbl_bckp

taxonomy_tbl %>% colnames() %>% paste0(collapse = "\n") %>% cat()

#fill NA tax with combination of max_tax and rank
for (line in 1:nrow(taxonomy_tbl)) {
  # if (taxonomy_tbl$genus[line] %in% c("NA",NA,"")) {
  if (taxonomy_tbl$superkingdom[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$superkingdom[line] <- paste0("superkingdom of ", taxonomy_tbl$kingdom[line]) }
  
  if (taxonomy_tbl$kingdom[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$kingdom[line] <- paste0("kingdom of ", taxonomy_tbl$superkingdom[line]) }
  
  if (taxonomy_tbl$phylum[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$phylum[line] <- paste0("phylum of ", taxonomy_tbl$kingdom[line]) }
  
  if (taxonomy_tbl$subphylum[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$subphylum[line] <- paste0("subphylum of ", taxonomy_tbl$phylum[line]) }
  
  if (taxonomy_tbl$class[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$class[line] <- paste0("class of ", taxonomy_tbl$subphylum[line]) }
  
  if (taxonomy_tbl$subclass[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$subclass[line] <- paste0("subclass of ", taxonomy_tbl$class[line]) }
  
  if (taxonomy_tbl$order[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$order[line] <- paste0("order of ", taxonomy_tbl$subclass[line]) }
  
  if (taxonomy_tbl$suborder[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$suborder[line] <- paste0("suborder of ", taxonomy_tbl$order[line]) }
  
  if (taxonomy_tbl$family[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$family[line] <- paste0("family of ", taxonomy_tbl$suborder[line]) }
  
  if (taxonomy_tbl$subfamily[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$subfamily[line] <- paste0("subfamily of ", taxonomy_tbl$family[line]) }
  
  # if (is.na(taxonomy_tbl$genus[line])) {
    # taxonomy_tbl$genus[line] <- paste0("genus of ", taxonomy_tbl$subfamily[line]) }
  
  ## Fiz isto para esta analise. Avaliar se o resultado funciona para a analise que voce esta fazendo agora!!
  
  if (is.na(taxonomy_tbl$genus[line])) {
  # Pega o primeiro nome da coluna Sci_name, que normalmente é o gênero
  taxonomy_tbl$genus[line] <- strsplit(taxonomy_tbl$Sci_name[line], " ")[[1]][1]
}
}

taxonomy_tbl %>% colnames() %>% paste0(collapse = '",\n"') %>% cat()

taxonomy_tbl <- taxonomy_tbl %>% 
  dplyr::rename(
    "Superkingdom (BLASTn)" = "superkingdom",
    "Kingdom (BLASTn)" = "kingdom",
    "Phylum (BLASTn)" = "phylum",
    "Subphylum (BLASTn)" = "subphylum",
    "Class (BLASTn)" = "class",
    "Subclass (BLASTn)" = "subclass",
    "Order (BLASTn)" = "order",
    "Suborder (BLASTn)" = "suborder",
    "Family (BLASTn)" = "family",
    "Subfamily (BLASTn)" = "subfamily",
    "Genus (BLASTn)" = "genus")

View(taxonomy_tbl)

# taxonomy_tbl_bckp2 <- taxonomy_tbl

#10- bind tax rank cols to DB_tbl ----
blast_res_tax <- left_join(x = blast_res_full, 
                           y = taxonomy_tbl,
                           by = "query_taxID")

View(blast_res_tax)

# Verificar quais nao possuem classificacao
blast_res_tax[is.na(blast_res_tax$`Superkingdom (BLASTn)`),] %>% 
  View()

blast_res_tax %>% filter(`Genus (BLASTn)` %in% c(NA)) %>% View()

blast_res_full %>% colnames()

saveRDS(object = blast_res_tax,
        file = paste0(results_path,"/blast_res_tax.rds"))

# Save environment
base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_BLAST_final.RData"))
```

### Combine BLAST results

```{r, eval=FALSE, echo=TRUE}

all_ps_tbl %>% unique() %>% View()

all_ps_tbl %>% colnames()

all_ps_tbl <- all_ps_tbl %>% 
  dplyr::rename("Sequence" = "ASV")

all_ps_tbl_blast <- left_join(x = all_ps_tbl,
                              y = blast_res_tax, 
                              by = "Sequence") %>%
  filter(Abundance > 0)

View(all_ps_tbl_blast)

colnames(all_ps_tbl_blast)
```

## Editing final results

### Remove uninformative columns from complete results table

```{r,echo=TRUE, eval=FALSE}

## If the identifications did not use DADA2 results, remove corresponding columns to make files and tables lighter

all_ps_tbl_blast %>% colnames() %>% paste0(collapse = '",\n"') %>% cat()

## Nao marque o que e para remover:
all_ps_tbl_blast <- all_ps_tbl_blast %>%
  dplyr::select(-c(
    # "Sample total abundance",
    # "Relative abundance to all samples",
    # "Relative abundance on sample",
    # "Sequence",
    # "Abundance",
    # "Project",
    # "Sample",
    # "Unique_File_name_Primer",
    # "Primer",
    # "Unique_File_name",
    # "Lib",
    # "Type",
    # "PCR.control",
    # Removendo resultados do DADA2
    "Kingdom",
    "Phylum",
    "Class",
    "Order",
    "Family",
    "Genus",
    "Species",
    "Specimen",
    # "Basin",
    # "Read origin",
    # "ASV tip",
    # "ASV Size (pb)",
    # "blast ID",
    # "blast ID Origin",
    # "query_taxID",
    # "max_tax",
    # "1_subject header",
    # "1_subject",
    # "1_indentity",
    # "1_length",
    # "1_mismatches",
    # "1_gaps",
    # "1_query start",
    # "1_query end",
    # "1_subject start",
    # "1_subject end",
    # "1_e-value",
    # "1_bitscore",
    # "1_qcovhsp",
    # "1_staxid",
    # "2_subject header",
    # "2_subject",
    # "2_indentity",
    # "2_length",
    # "2_mismatches",
    # "2_gaps",
    # "2_query start",
    # "2_query end",
    # "2_subject start",
    # "2_subject end",
    # "2_e-value",
    # "2_bitscore",
    # "2_qcovhsp",
    # "2_staxid",
    # "3_subject header",
    # "3_subject",
    # "3_indentity",
    # "3_length",
    # "3_mismatches",
    # "3_gaps",
    # "3_query start",
    # "3_query end",
    # "3_subject start",
    # "3_subject end",
    # "3_e-value",
    # "3_bitscore",
    # "3_qcovhsp",
    # "3_staxid",
    # "1_DB",
    # "2_DB",
    # "3_DB",
    # "Sci_name",
    # "Superkingdom (BLASTn)",
    # "Kingdom (BLASTn)",
    # "Phylum (BLASTn)",
    # "Subphylum (BLASTn)",
    # "Class (BLASTn)",
    # "Subclass (BLASTn)",
    # "Order (BLASTn)",
    # "Suborder (BLASTn)",
    # "Family (BLASTn)",
    # "Subfamily (BLASTn)",
    # "Genus (BLASTn)"
    ))

colnames(all_ps_tbl_blast)

```

### Final ID

```{r,echo=TRUE, eval=FALSE}

#all_ps_tbl_blast_bckp2 <- all_ps_tbl_blast
#all_ps_tbl_blast <- all_ps_tbl_blast_bckp2 

# DADA2 final identification ----

# all_ps_tbl_blast <- all_ps_tbl_blast %>%
#   mutate("Final ID (DADA2)" = if_else((`exact Species (DADA2)` %in% c(NA,"NA", "NA NA")),
#                               if_else((`Species (DADA2)` %in% c(NA,"NA")),
#                                       if_else(`Genus (DADA2)` %in% c(NA,"NA"),
#                                               # if_else((`blast ID` %in% c(NA,"NA")),
#                                                       # if_else((Subfamily %in% c(NA,"NA")),
#                                                               if_else((`Family (DADA2)` %in% c(NA,"NA")),
#                                                                       # if_else((Suborder %in% c(NA,"NA")),
#                                                                               if_else((`Order (DADA2)` %in% c(NA,"NA")),
#                                                                                       `Class (DADA2)`,
#                                                                                       `Order (DADA2)`),
#                                                                               # Suborder),
#                                                                       `Family (DADA2)`),
#                                                               # Subfamily),
#                                                      # `blast ID`),
#                                               `Genus (DADA2)`),
#                                       `Species (DADA2)`),
#                               as.character(`Exact GenSp (DADA2)`)))


# BLASTn final identification ----

all_ps_tbl_blast <- all_ps_tbl_blast %>%
           mutate("Final ID (BLASTn)" = `blast ID`) 

all_ps_tbl_blast$`Final ID (BLASTn)` %>% unique() %>%  sort()

View(all_ps_tbl_blast)

```

### ASVs seqs

```{r,echo=TRUE, eval=FALSE}

#25 - recover all ASVs sequences to prepare fasta ----

colnames(all_ps_tbl_blast)[colnames(all_ps_tbl_blast) == "ASV"] <- "ASV (Sequence)"
# names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "ASV Size (pb)"

#all ----
# giving our seq headers more manageable names (ASV_1, ASV_2...)
all_asv_seqs <- tibble("Sequence" = unique(all_ps_tbl_blast$Sequence))

View(all_asv_seqs)

all_asv_seqs <- all_asv_seqs %>%
  mutate("ASV length" = nchar(`Sequence`)) %>% 
  arrange(desc(`ASV length`)) %>% 
  mutate("ASV header" = paste0(">ASV_",row_number(.),"_",`ASV length`, "bp"))

View(all_asv_seqs)

# Verificar se o tamanho corresponde ao numero inicial de ASVs
all_asv_seqs$`Sequence` %>% unique() %>% length() # 7982 esta certo!

#combine ASV headers and all_ps_tbl
all_ps_tbl_blast <- dplyr::left_join(x = all_ps_tbl_blast,
                                     y = all_asv_seqs[,c("Sequence","ASV header")],
                                     by = "Sequence")
all_ps_tbl_blast %>% colnames()

# making and writing out a fasta of our final ASV seqs with tax

log10(all_ps_tbl_blast$Abundance) %>% table()  %>%  plot()
all_ps_tbl_blast$`Relative abundance on sample` %>% table()  %>%  plot()
log10(all_ps_tbl_blast$`Relative abundance on sample`/100) %>% table()  %>%  plot()
(all_ps_tbl_blast$`Relative abundance on sample`) %>% table()  %>%  plot()

all_ps_tbl_blast %>% unique() %>% View()
all_ps_tbl_blast %>% duplicated() %>% View()
all_ps_tbl_blast[all_ps_tbl_blast %>% duplicated(),]

all_ps_tbl_blast$Abundance %>% table() %>%  plot()
all_ps_tbl_blast$`Relative abundance on sample` %>% table() %>%  plot()
all_ps_tbl_blast$`ASV Size (pb)` %>% table() %>%  plot()
```

## SWARM - ASVs to OTUs

```{r,echo=TRUE, eval=FALSE}

asvs_abd <- all_ps_tbl_blast %>%
  # filter(!`Read origin` %in% c("concat")) %>% 
  filter(!str_detect(string = `Sequence`,pattern = "N")) %>% 
  dplyr::select(c("Sequence","ASV header","Abundance")) %>% 
  group_by(`Sequence`,`ASV header`) %>%
  mutate("ASV total abundance" = sum(Abundance)) %>%
  dplyr::select(c(`Sequence`,`ASV header`,`ASV total abundance`)) %>%
  unique() %>%
  mutate(`ASV header abd` = paste0(`ASV header`,"_",`ASV total abundance`))

# write fasta file with ASVs and Taxonomy
all_asv_fasta_abd <- c(rbind(asvs_abd$`ASV header abd`, asvs_abd$`Sequence`))

write(all_asv_fasta_abd, paste0(results_path,"/",prjct_rad,"-ASVs_abd.fasta"))

paste0(results_path,"/",prjct_rad,"-ASVs_abd.fasta")
```

#### Run SWARM V2 on command line

```{r ,echo=TRUE, eval=FALSE}

# 1 - run SWARM

system2(command = "swarm", args = c(paste0(
  paste0(results_path,"/",prjct_rad,"-ASVs_abd.fasta "),
                                   " -t 50 "," -f ",
                                   " -s ", paste0(results_path,"/swarm/",prjct_rad,"-swarm.stats "),
                                   " -o ", paste0(results_path,"/swarm/",prjct_rad,"-swarm.out "),
                                   " -w ", paste0(results_path,"/swarm/",prjct_rad,"-representative_OTUs.fasta "),
                                   " -i ", paste0(results_path,"/swarm/",prjct_rad,"-swarm.structure ")
                                   )),) # guarantee no zerolength reads

# detect files generated by swarm and locate OTUs output
swarm_clust <- list.files(path = swarm_path,
                          pattern = "swarm.out",
                          full.names = TRUE ) %>% 
readr::read_lines()

swarm_clust

find_otu <- function(ASV_header, clusters_swarm){
  ASV_OTU_tbl <- tibble::tibble(`ASV header abd` = stringr::str_remove_all(string = ASV_header,
                                                                           pattern  = ">"),
                                OTU = 0)
  ASV_OTU_tbl$OTU <- which(grepl(x = clusters_swarm,
                                 pattern = ASV_OTU_tbl$`ASV header abd`))
                                     
 return(ASV_OTU_tbl)
} 

# Versões paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78

future::plan(future::multisession(workers = cores_to_be_used))

ASVs_and_OTUs <- furrr::future_map_dfr(.x = asvs_abd$`ASV header abd`,
                                       clusters_swarm = swarm_clust,
                                       .f = find_otu,
                                       .options = furrr::furrr_options(seed = NULL))

ASVs_and_OTUs_bckp <- ASVs_and_OTUs
# ASVs_and_OTUs <- ASVs_and_OTUs_bckp

ASVs_and_OTUs$`ASV header abd` <- 
  ASVs_and_OTUs$`ASV header abd` %>% 
  str_replace(pattern = "^",replacement = ">") 

colnames(ASVs_and_OTUs)

  
# ASVs_and_OTUs$`ASV header abd` <- ASVs_and_OTUs$`ASV header abd` %>% str_replace(pattern = ">>",replacement = "")

ASVs_and_OTUs$OTU %>% unique() %>% length()

# asvs_abd <- asvs_abd %>% dplyr::select(-c("OTU.x", "OTU.y"))

asvs_abd <- left_join(asvs_abd,
                      ASVs_and_OTUs,
                      by = "ASV header abd")
View(asvs_abd)

# Verificar numero de ASVs
asvs_abd$Sequence %in% all_ps_tbl_blast$Sequence %>% sum()

all_ps_tbl_blast <- left_join(x = all_ps_tbl_blast,
                              y = asvs_abd[,c("Sequence","OTU")],
                              by = "Sequence") 
View(all_ps_tbl_blast)

colnames(all_ps_tbl_blast)

# rename column
names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast) == "Sequence")] <- "ASV (Sequence)"

all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU) %>% View() 
all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU,`ASV Size (pb)`) %>% unique() %>% View() 
all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU,`ASV Size (pb)`,`Read origin`) %>% unique() %>% View() 
all_ps_tbl_blast %>% dplyr::select(`ASV (Sequence)`,`Final ID (BLASTn)`,OTU,`ASV Size (pb)`,`Read origin`) %>% unique() %>% View() 

# all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU) %>% dplyr::select(OTU) %>% unique() 
all_ps_tbl_blast %>% dplyr::select(`ASV (Sequence)`,`Final ID (BLASTn)`,OTU) %>% unique() %>% View()

all_asv_seqs_tax <- all_ps_tbl_blast %>%
  dplyr::select(c("ASV (Sequence)", "Primer", "ASV header",
           "Class (BLASTn)", "Family (BLASTn)", "Genus (BLASTn)", "blast ID")) %>% 
  mutate(`ASV header` = str_remove_all(string = `ASV header`,pattern = ">")) %>% 
  unique() %>% 
  group_by(`ASV (Sequence)`,`ASV header`) %>%
  mutate(`Full ASV header` = paste0(">ENVIRONMENTAL: ",(paste0(c(`ASV header`,`Primer`, `Class (BLASTn)`, `Family (BLASTn)`, `Genus (BLASTn)`, `blast ID`),collapse = ";")))) %>%
  ungroup() %>% 
  unique() 

View(all_asv_seqs_tax)

#write fasta file with ASVs and Taxonomy
all_asv_fasta <- c(rbind(all_asv_seqs_tax$`Full ASV header`, all_asv_seqs_tax$`ASV (Sequence)`))

write(all_asv_fasta, paste0(results_path,"/",prjct_rad,"-all_ASVs.fasta"))
```

## Identify ASVs present on the Blanks/Negative controls

```{r,echo=TRUE, eval=FALSE}

#corrigindo que na tabela os controles não foram importados

all_ps_tbl_blast$Sample[all_ps_tbl_blast$Sample %>% grepl(pattern = c("Neg|Bco|Pos"),ignore.case = T)] %>% unique()
all_ps_tbl_blast$Unique_File_name[all_ps_tbl_blast$Unique_File_name %>% grepl(pattern = c("Neg|Bco"),ignore.case = T)] %>% unique()

all_ps_tbl_blast$PCR.control %>% unique() %>% str_split(pattern = ";",simplify = T) %>% c() %>%  unique() %>% sort()

all_ps_tbl_blast$Sample %>% unique()
all_ps_tbl_blast$Type %>% unique()
all_ps_tbl_blast %>% unique()
all_ps_tbl_blast %>% colnames()

# Marcando ASVs presentes nos controles
all_ps_tbl_blast <- all_ps_tbl_blast %>%
  dplyr::mutate("Remove" = case_when(Type %in% c("Extraction control",
                                                 "Extraction Control",
                                                 "PCR control",
                                                 "PCR Control",
                                                 "Filtration control",
                                                 "Filtration Control",
                                                 "Negative control",
                                                 "Negative Control",
                                                 "Positive control",
                                                 "Positive Control",
                                                 "Control") ~ "Controls",
                                     TRUE ~ "ASV exclusive to samples"))

# Numero de observações por grupo
all_ps_tbl_blast$Remove %>% table()

# Identify contamination based on respective controls----
all_ps_tbl_blast$Unique_File_name %>% unique() %>% sort()
all_ps_tbl_blast$PCR.control%>% unique() %>% sort()
# all_ps_tbl_blast$Filtration.control%>% unique() %>% sort()

#create table with controls only 
all_contam_ASVs <- all_ps_tbl_blast %>%
  filter(Abundance > 0) %>% 
  filter(Remove %in% "Controls") %>%
  group_by(`ASV (Sequence)`, Unique_File_name) %>% 
  mutate("Max. ASV abd. in control" = max(`Relative abundance on sample`)) %>%
  ungroup() %>%
  dplyr::select("Unique_File_name",
                "ASV (Sequence)",
                "Max. ASV abd. in control",
                "Final ID (BLASTn)"
                # ,"Final ID (DADA2)"
                ) %>%
  unique()

all_contam_ASVs$Unique_File_name %>% unique()
all_ps_tbl_blast$Unique_File_name %>% unique()
all_contam_ASVs$`Final ID (BLASTn)` %>% unique()

View(all_contam_ASVs)

# Is there any of the ASVs in control also in the Samples?
all_contam_ASVs$`ASV (Sequence)` %in% (all_ps_tbl_blast$`ASV (Sequence)` %>% unique())
all_ps_tbl_blast[((all_ps_tbl_blast$`ASV (Sequence)`) %in% all_contam_ASVs$`ASV (Sequence)`),] %>% View()

# save new complete table to edit controls
all_ps_tbl_blast_controls <- all_ps_tbl_blast %>% 
  dplyr::mutate("Primer" = str_remove(Unique_File_name_Primer,
                                      pattern = paste0("--",Unique_File_name))) %>% 
  relocate(Primer) %>% 
  dplyr::mutate("Prop. to PCR control" = 0,
                "Prop. to Ext control" = 0,
                "Prop. to Filt control" = 0)

View(all_ps_tbl_blast_controls)

#mark contam ASVs in all other samples

# compare sample table with control table and assign foldchanges

all_ps_tbl_blast_controls[all_ps_tbl_blast_controls$`ASV (Sequence)` %in% (all_contam_ASVs$`ASV (Sequence)`),] %>% View()

# Verificar nomes dos controles na tabela de contaminacoes 
all_contam_ASVs$Unique_File_name %>% unique()

# Comparar os nomes dos controles acima
all_ps_tbl_blast_controls$PCR.control %>% unique()
all_ps_tbl_blast_controls$Filtration.control %>% unique()
all_ps_tbl_blast_controls$Extraction.control %>% unique()

# CORRECAO Adicionando os nomes corretos da coluna PCR.control #Ignorar se os nomes acima forem iguais
{
  # primers_n_samples_PCR.control <- primers_n_samples %>% select(`PCR control`, Sample)

  # all_ps_tbl_blast_controls <- left_join(all_ps_tbl_blast_controls,
  #                                        primers_n_samples_PCR.control,
  #                                        by = "Sample")
  # all_ps_tbl_blast_controls$PCR.control <- all_ps_tbl_blast_controls$`PCR control`
  # 
  # all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% select(-`PCR control`)
}

all_ps_tbl_blast_controls$Type %>% unique()
all_ps_tbl_blast_controls$Unique_File_name[all_ps_tbl_blast_controls$Type %in% c("PCR Control")] %>% unique()
all_ps_tbl_blast_controls$Unique_File_name[all_ps_tbl_blast_controls$Type %in% c("Filtration control")] 
all_ps_tbl_blast_controls$Unique_File_name[all_ps_tbl_blast_controls$Type %in% c("Filtration Control")] 

# Adicionando colunas que nao foram incluidas
all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% mutate("Filtration.control" = "",
                                                                  "Extraction.control" = "")
all_ps_tbl_blast_controls %>% colnames()

# compare sample table with control table and assign foldchanges
for (line in 1:nrow(all_ps_tbl_blast_controls)) {
  if(all_ps_tbl_blast_controls$`ASV (Sequence)`[line] %in% (all_contam_ASVs$`ASV (Sequence)`)){
    seq2search <- all_ps_tbl_blast_controls$`ASV (Sequence)`[line]
    file2search <- all_ps_tbl_blast_controls$Unique_File_name[line]
    PCRcontrols2search <- all_ps_tbl_blast_controls$`PCR.control`[line] %>% str_split(pattern = ";") %>% unlist()
    FILTcontrols2search <- all_ps_tbl_blast_controls$`Filtration.control`[line] %>% str_split(pattern = ";") %>%
      unlist()
    EXTcontrols2search <- all_ps_tbl_blast_controls$`Extraction.control`[line] %>% str_split(pattern = ";") %>%
      unlist()
    PCR_control_tbl <- all_contam_ASVs %>% filter(Unique_File_name %in% PCRcontrols2search & `ASV (Sequence)` %in%
                                                    seq2search)
     # browser()
    FILT_control_tbl <- all_contam_ASVs %>% filter(Unique_File_name %in% FILTcontrols2search & `ASV (Sequence)` %in%
                                                     seq2search)
    EXT_control_tbl <- all_contam_ASVs %>% filter(Unique_File_name %in% EXTcontrols2search & `ASV (Sequence)` %in%
                                                    seq2search)
    #proportion on PCR.control
    all_ps_tbl_blast_controls$`Prop. to PCR control`[line] <- gtools::foldchange(denom = max(PCR_control_tbl$`Max. ASV abd. in control`),
                                                                                 num = all_ps_tbl_blast_controls$`Relative abundance on sample`[line])
    #proportion on Filt.control
    all_ps_tbl_blast_controls$`Prop. to Filt control`[line] <- gtools::foldchange(denom = max(FILT_control_tbl$`Max. ASV abd. in control`),
                                                                                  num = all_ps_tbl_blast_controls$`Relative abundance on sample`[line])
    #proportion on Extraction.control
    all_ps_tbl_blast_controls$`Prop. to Ext control`[line] <- gtools::foldchange(denom = max(EXT_control_tbl$`Max. ASV abd. in control`),
                                                                                 num = all_ps_tbl_blast_controls$`Relative abundance on sample`[line])
  }
  print(line)
  }

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% mutate("Possible contamination" = "True detection")

# mark possible contaminations
all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>%
  mutate(`Possible contamination` = case_when(
    (all_ps_tbl_blast_controls$`Prop. to PCR control` != 0) ~ "Possible contamination",
                                              (all_ps_tbl_blast_controls$`Prop. to Filt control` != 0) ~ "Possible contamination",
                                              (all_ps_tbl_blast_controls$`Prop. to Ext control` != 0) ~ "Possible contamination",
      TRUE ~ "True detection"
      ))
  
all_ps_tbl_blast_controls$`Possible contamination` %>% table()

all_ps_tbl_blast_controls$`Prop. to PCR control` %>% table() %>% plot()

all_ps_tbl_blast_controls %>% 
  filter(`Possible contamination` %in% c("Possible contamination")) %>% View()

all_ps_tbl_blast_controls$Unique_File_name_Primer
all_ps_tbl_blast_controls$Primer

all_ps_tbl_blast_controls %>% 
  filter(Type %in% c("PCR control")) %>% View()

all_ps_tbl_blast$`ASV (Sequence)` %>%  unique()
all_ps_tbl_blast$Sample %>%  table()
all_ps_tbl_blast$Remove %>%  table()

View(all_ps_tbl_blast_controls)

```

## Plot identified ASVs

```{r, eval=FALSE}

all_ps_tbl_blast_controls$`Superkingdom (BLASTn)` %>% unique()
all_ps_tbl_blast_controls$Project%>% unique()

library(ggtext)

for (project in all_projects) {

project_name <- project %>% str_replace_all(pattern = " ",
                                        replacement = "_")

N_samples <-   all_ps_tbl_blast_controls %>%
  filter(str_detect(string = Project, pattern = project)) %>% 
  pull(Unique_File_name) %>% unique() %>% length()

max_ASV <- all_ps_tbl_blast_controls %>%
  filter(str_detect(string = Project, pattern = project)) %>% 
  pull(`ASV Size (pb)`)  %>% max()

all_ps_tbl_blast_controls$Primer %>% unique()

library(writexl)

write_xlsx(all_ps_tbl_blast_controls, "/home/gabriel/projetos/analises/metagen/MTG05_0924/data/all_ps_tbl_blast_controls.xlsx")

ASV_length_by_Sample_BLAST <- all_ps_tbl_blast_controls  %>%
  arrange(Primer) %>% 
  # filter(str_detect(string = Project, pattern = project)) %>%
  # filter(!`Read origin` %in% "merged") %>%
  # filter(`ASV Size (pb)` %in% c(265:280)) %>%
  # filter(!`Possible contamination` %in% c("True detection")) %>%
  filter(`Possible contamination` %in% c("True detection")) %>%
  filter(Type %in% c("Sample")) %>%
  # mutate(Sample=factor(Sample,levels = sample_levels)) %>% 
  mutate("BLASTn pseudo-score" = (`1_indentity`*`1_qcovhsp`/100)) %>%
  # arrange(desc(`BLASTn pseudo-score`)) %>% 
  arrange(desc(is.na(`BLASTn pseudo-score`))) %>%
  # ggplot(aes(y=Sample,
  ggplot(aes(y=interaction(Unique_File_name_Primer,
                           # `Metadata 1`,
                           sep = " - "),
             # ggplot(aes(y=interaction(`Read origin`,Sample,sep = " - "),
             x=`ASV Size (pb)`,
             fill = `BLASTn pseudo-score`,
             # col = `BLASTn pseudo-score`,
             col = "#c2c2c2",
             shape = `Superkingdom (BLASTn)`,
             size =`Relative abundance on sample`,
             alpha = 0.2,
             group = `Final ID (BLASTn)`
             )) +
  geom_jitter(height = 0.3,
              col = "#c2c2c2",
              width = 0.5) +
  scale_fill_gradientn(name = "BLASTn\nidentification\n _pseudo-score_ (%)",
                       na.value = "grey95",
                       colours = c("dark red","red","yellow","green","dark green"),
                       values = c(0,1),
                       breaks = c(60,65,70,75,80,85,90,95,100))+
    # scale_color_gradientn(name = "BLASTn\nidentification\n _pseudo-score_ (%)",
    #                      na.value = "grey80",
    #                    colours = c("dark red","red","yellow","green","dark green"),
    #                    values = c(0,1),
    #                    breaks = c(60,65,70,75,80,85,90,95,100))+
  scale_size_continuous(name = "Abundância\n     relativa\nna amostra (%)",
                        breaks = c(0,1,10,20,30,40,50,60,70,80,90,100),
                        ) +
  scale_x_continuous(breaks = c(seq(20,
                                    max(all_ps_tbl$`ASV Size (pb)`),10)),
  # scale_x_continuous(breaks = c(seq(10,300,10)),
                     expand = c(0.02,0.02),
                     sec.axis = dup_axis()) +
  scale_shape_manual(name = "Superkingdom (BLASTn)",
                     # values = c(25,24,21,23),na.value = 22
                     # values = c(24,21,23),na.value = 22
                     values = c(21,23),na.value = 22
                     ) +
  xlab("Tamanho da ASV (pb)") +
  ylab("Amostra") +
  ggtitle(label = paste0("GMB - ",project),
          subtitle = "Status de identificação de todas ASVs encontradas na análise, após remoção das ASVs dos controles negativos") +
  theme_bw(base_size = 30) +
  theme(legend.position = "right")+
  theme(axis.text.x = element_text(angle = 0,hjust = 0.5),
        legend.title = element_markdown()
        ) +
  guides(alpha="none",
         color="none") +
  theme(strip.text = element_text(size = 24),
        axis.title = element_text(size = 24),
        axis.text.x = element_text(size = 24),
        legend.position = "bottom",
        legend.key.width = unit(4, 'cm'),
        legend.key.height = unit(0.5, 'cm')) +
  facet_grid(rows = vars(Primer),
  # facet_grid(rows = vars(Project,Primer),
             space = "free_y",
             scales = "free_y")
# + 
#   annotate(geom = "rect",
#            xmin = -Inf,
#            # xmax = 164,
#            xmax = 85,
#            ymin = 0,
#            ymax = Inf,
#            fill = "#ff0033",
#            alpha = 0.075) +
#   annotate(geom = "rect",
#            xmin = 150,
#            # xmax = 164,
#            xmax = Inf,
#            ymin = 0,
#            ymax = Inf,
#            fill = "#ff0033",
#            alpha = 0.075)
# +
#   annotate(geom = "rect",
#            # xmin = 179,
#            xmin = 220,
#            xmax = Inf,
#            ymin = 0,
#            ymax = Inf,
#            fill = "#ff0033",
#            alpha = 0.075)


ASV_length_by_Sample_BLAST


ggsave(file = paste0(figs_path,"/",project_name,"-ASV_length_by_sample-ALL-ASVs-noContams.pdf",collapse = ""),
# ggsave(file = paste0(results_path,"/",
#                      unique(smp_abd_ID_Final$Project),"/",
#                      unique(smp_abd_ID_Final$Project),"-ASV_length_by_sample-ALL-ASVs.pdf",collapse = ""),
     plot = ASV_length_by_Sample_BLAST,
     device = "pdf",
     width = 140,
     height = ifelse(N_samples <= 10, 50, round(N_samples/0.75)), limitsize=FALSE,
     units = "cm",
     dpi = 600)

}

library(plotly)

ASV_length_by_Sample_BLAST_plotly <- ASV_length_by_Sample_BLAST %>% ggplotly(tooltip = c("Final ID (BLASTn)",
                                                                                         "Sample",
                                                                                         "ASV Size (pb)",
                                                                                         "BLASTn pseudo-score",
                                                                                         "Superkingdom (BLASTn)",
                                                                                         "Relative abundance on sample"))

htmlwidgets::saveWidget(widget = ASV_length_by_Sample_BLAST_plotly,
                selfcontained = TRUE,
                        file = paste0(figs_path,"/",prjct_rad,"-ASVs_BLASTn_IDed_interactive-noContams.html"))
```

## ASV expected length per primer

```{r, eval=FALSE, echo=TRUE}

all_ps_tbl_blast_controls_bckp <- all_ps_tbl_blast_controls
# all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls_bckp

# fill ranges column with expected primer insert ranges

all_ps_tbl_blast_controls$Primer %>% unique() 

all_ps_tbl_blast_controls %>% colnames()

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
dplyr::mutate(`Expected length` = "FALSE")

all_ps_tbl_blast_controls$Primer %>% unique() %>% sort() %>% 
  paste0(collapse = '",\n"') %>% cat()

all_ps_tbl_blast_controls %>% 
  relocate("Class (BLASTn)",
           "Primer",
           "Abundance",
           "Type",
           "Read origin",
           "ASV Size (pb)",
           "max_tax") %>% 
  View()

# Avaliar distribuicao do tamanho das reads para definir Expected length
# Usando os graficos feitos na sessao Plot identified ASVs
# Use a janela que tem IDs confiáveis dos grupos alvo

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  dplyr::mutate(`Expected length` = case_when(
    (Primer %in% "MiFish" & `ASV Size (pb)` %in% c(161:180) &
      `Read origin` %in% c("merged")) ~ "in range",
    # (Primer %in% "ITS2" & `ASV Size (pb)` %in% c(420:455) &
      # `Read origin` %in% c("merged")) ~ "in range",
    # (Primer %in% "COI" & `ASV Size (pb)` %in% c(410:440) &
      # `Read origin` %in% c("merged")) ~ "in range",
    # (`Read origin` %in% c("R1", "R2")) ~ "in range",
  # (Primer %in%  "VF2_FR1d;Fish1;Fish2" & `ASV Size (pb)` %in% c(265:280) &
  # `Read origin` %in% c("R1","R2")) ~ "in range",
  # (Primer %in%  "VF2_FR1d;Fish1;Fish2") ~ "in range"
  # (Primer %in%  "VF2_FR1d;Fish1;Fish2" & `Read origin` %in% c("merged")) ~ "in range"
  # (Primer %in% c("VF2_FR1d","Fish1","Fish2") & `Read origin` %in% c("merged")) ~ "in range",
  # (Primer %in%  c("COIr1","COI_R1") & `ASV Size (pb)` %in% c(125:145)) ~ "in range",
  # (Primer %in%  "COI_R1;COI fwh2" & `ASV Size (pb)` %in% c(120:145,200:220)) ~ "in range",
  # (Primer %in%  "p16S" & `ASV Size (pb)` %in% c(110:165)) ~ "in range",
  # (Primer %in%  "p12SV5" & `ASV Size (pb)` %in% c(85:125)) ~ "in range",
  # (Primer %in%  "miniCOI" & `ASV Size (pb)` %in% c(160:190)) ~ "in range",
  # (Primer %in%  "MiFish" & `ASV Size (pb)` %in% c(160:180)) ~ "in range",
  # (Primer %in%  "MiBird" & `ASV Size (pb)` %in% c(155:195)) ~ "in range",
  # (Primer %in%  c("MiBird", "MiBird;MiMa", "MiFish", "MiFish;MiBird", "Mima") &
    # `ASV Size (pb)` %in% c(155:190)) ~ "in range",
  # (Primer %in%  c("12SV5", "12SV5;tay16SMam", "tay16SMam","p12SV5;taylor") &
    # `ASV Size (pb)` %in% c(75:150)) ~ "in range",
  # (Primer %in%  c("p12SV5;taylor") &
    # `ASV Size (pb)` %in% c(85:140)) ~ "in range",
    TRUE ~ "out of range"
    ))
    
all_ps_tbl_blast_controls %>% 
  select(`ASV Size (pb)`, Primer, `ASV header`, `Read origin`, `Expected length`) %>% View()

all_ps_tbl_blast_controls$`Expected length`[all_ps_tbl_blast_controls$`Expected length` == "FALSE"]

all_ps_tbl_blast_controls$`Expected length` %>% table()

all_ps_tbl_blast_controls %>% colnames() %>% sort()
```

## Set curated ID

The curated identification is obtained by manually (but programatically) correcting species based on biological scientific expertise, or species names that are uncorrect.

```{r,echo=TRUE, eval=FALSE}

#  determinar rank taxonomico mais confiável ----

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  mutate("BLASTn pseudo-score" = `1_indentity` *`1_qcovhsp` /100) %>% 
  mutate("Identification" =  case_when(`BLASTn pseudo-score` >= 98 ~ `Final ID (BLASTn)`,
                                       `BLASTn pseudo-score` >= 95 & `BLASTn pseudo-score` < 98 ~ paste(`Genus (BLASTn)`, "sp."),
                                       `BLASTn pseudo-score` >= 90 & `BLASTn pseudo-score` < 95 ~ `Family (BLASTn)`,
                                       `BLASTn pseudo-score` >= 80 & `BLASTn pseudo-score` < 90 ~ `Order (BLASTn)`,
                                       `BLASTn pseudo-score` >= 60 & `BLASTn pseudo-score` < 80 ~ `Class (BLASTn)`),
         "Identification Max. taxonomy" =               case_when(`BLASTn pseudo-score` >= 98 ~ "Species",
                                       `BLASTn pseudo-score` >= 95 & `BLASTn pseudo-score` < 98 ~ "Genus",
                                       `BLASTn pseudo-score` >= 90 & `BLASTn pseudo-score` < 95 ~ "Family",
                                       `BLASTn pseudo-score` >= 80 & `BLASTn pseudo-score` < 90 ~ "Order",
                                       `BLASTn pseudo-score` >= 60 & `BLASTn pseudo-score` < 80 ~ "Class")) %>% 
  relocate("Identification","Identification Max. taxonomy","BLASTn pseudo-score")

# Definindo Curated ID igual a Identification
all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  mutate("Curated ID" = Identification)

all_ps_tbl_blast_controls$`blast ID` %>% unique() %>% sort() %>% paste0(collapse = '",\n"') %>% cat()

{
# all_ps_tbl_blast_controls$`Curated ID`[(all_ps_tbl_blast_controls$`Curated ID` %in% c("Human DNA","Eukaryotic synthetic"))] <- "Homo sapiens"
# all_ps_tbl_blast_controls$`Curated ID`[(all_ps_tbl_blast_controls$`Curated ID` %in% c("Hydrochaeris hydrochaeris"))] <- "Hydrochoerus hydrochaeris"
# all_ps_tbl_blast_controls$`Curated ID`[(all_ps_tbl_blast_controls$`Curated ID` %in% c("Bacterium "))] <- "Bacterium "
}

all_ps_tbl_blast_controls$Remove %>% unique()

all_ps_tbl_blast_controls$`Curated ID` %>% unique() %>% sort() %>% paste0(collapse = '",\n"') %>% cat()

all_ps_tbl_blast_controls %>% select(`Curated ID`,OTU,Remove) %>% unique() %>% View()
```

## Mark Metazoans

```{r, eval=FALSE}

# Mark oossible Metazoan IDs ----

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  mutate("Possible Metazoa" = NA)

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>%
  mutate("Possible Metazoa" = case_when(stringr::str_detect(string =`Kingdom (BLASTn)`, pattern = "acter|Archaea|Virus|Fungi|ridiplantae") ~ FALSE,
                                        is.na(`Kingdom (BLASTn)`) ~ FALSE,
                                        stringr::str_detect(string =`Kingdom (BLASTn)`, pattern = "Metazoa") ~ TRUE, TRUE ~ FALSE))

all_ps_tbl_blast_controls %>%
  filter(`Possible Metazoa` == FALSE) %>% 
  select(`Phylum (BLASTn)`) %>% 
  table()

all_ps_tbl_blast_controls %>% filter(`Possible Metazoa` %in% c(NA,"NA")) %>% View()
all_ps_tbl_blast_controls %>% filter(`Possible Metazoa` %in% c(FALSE)) %>% View()
all_ps_tbl_blast_controls %>% filter(`Possible Metazoa` %in% c(TRUE)) %>% View()

```

## Reorder table

```{r, eval=FALSE}

all_ps_tbl_blast_controls %>% colnames() %>% paste0(collapse = '",\n"') %>% cat()

all_ps_tbl_blast_controls <-
  all_ps_tbl_blast_controls %>%
  dplyr::rename("PCR control" = "PCR.control",
                "Ext. control"= "Extraction.control",
                "Filt. control" = "Filtration.control") %>%
  dplyr::select(c(
    # "Researcher",
    "Project",
    "Primer",
    # "Run",
    # "Analysis",
    "Sample",
    "Unique_File_name_Primer",
    "Unique_File_name",
    "Read origin",
    "Relative abundance to all samples",
    "Relative abundance on sample",
    "Sample total abundance",
    "Abundance",
    starts_with(match = "Metadata"),
    # "obs",
    "BLASTn pseudo-score",
    "Identification",
    "Identification Max. taxonomy",
    "Curated ID",
    # "Final ID ",
    "Final ID (BLASTn)",
    # "Final ID (BOLD)",
    # "Final ID (DADA2)",
    "Expected length",
    "Possible Metazoa",
    "blast ID",
    "blast ID Origin",
    #DADA2
    # "Exact GenSp (DADA2)",
    # "exact Species (DADA2)",
    # "Exact Genus (DADA2)",
    # # "Group",
    # # "Group (DADA2 bootstrap)",
    # "Basin (DADA2)",
    # "Basin (DADA2 bootstrap)",
    # "Species (DADA2)",
    # "Species (DADA2 bootstrap)",
    # "Specimen (DADA2)",
    # "Specimen (DADA2 bootstrap)",
    # "Genus (DADA2)",
    # "Genus (DADA2 bootstrap)",
    # "Family (DADA2)",
    # "Family (DADA2 bootstrap)",
    # "Order (DADA2)",
    # "Order (DADA2 bootstrap)",
    # "Class (DADA2)",
    # "Class (DADA2 bootstrap)",
    # "Phylum (DADA2)",
    # "Phylum (DADA2 bootstrap)",
    # "Kingdom (DADA2)",
    # "Kingdom (DADA2 bootstrap)",
          #BLAST
    # "max_tax",
    "Genus (BLASTn)",
    "Subfamily (BLASTn)",
    "Family (BLASTn)",
    "Suborder (BLASTn)",
    "Order (BLASTn)",
    "Subclass (BLASTn)",
    "Class (BLASTn)",
    "Phylum (BLASTn)",
    "Subphylum (BLASTn)",
    "Kingdom (BLASTn)",
    "Superkingdom (BLASTn)",
    # "Kingdom (BLASTn)",
    # "1_res",
    "1_subject header",
    # "1_query",
    "1_staxid",
    "1_subject",
    "1_indentity",
    "1_qcovhsp",
    "1_length",
    "1_mismatches",
    "1_gaps",
    "1_query start",
    "1_query end",
    "1_subject start",
    "1_subject end",
    "1_e-value",
    "1_bitscore",
    # "2_res",
    "2_subject header",
    "2_staxid",
    # "2_query",
    "2_subject",
    "2_indentity",
    "2_qcovhsp",
    "2_length",
    "2_mismatches",
    "2_gaps",
    "2_query start",
    "2_query end",
    "2_subject start",
    "2_subject end",
    "2_e-value",
    "2_bitscore",
    # "3_res",
    "3_subject header",
    "3_staxid",
    # "3_query",
    "3_subject",
    "3_indentity",
    "3_qcovhsp",
    "3_length",
    "3_mismatches",
    "3_gaps",
    "3_query start",
    "3_query end",
    "3_subject start",
    "3_subject end",
    "3_e-value",
    "3_bitscore",
    # ends_with(match = "(BOLD)"),
    "Remove",
    "ASV Size (pb)",
    "ASV header",
    "ASV (Sequence)",
    # "Sequence",
    "OTU",
    "Ext. control",
    "PCR control",
    "Filt. control",
    "Prop. to PCR control",
    # "Prop. to Ext control",
    # "Prop. to Filt control",
    "Possible contamination",
    "Type"
    # "Run"
    ))

View(all_ps_tbl_blast_controls)

all_ps_tbl_blast_controls$`Curated ID` %>%  unique() %>% sort()

#Save environment
base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_reorder-tbl.RData"))
```

## Recalculate clean abundances

```{r,echo=TRUE, eval=FALSE}

all_ps_tbl_blast_controls %>% 
  select(Unique_File_name,Sample) %>% View()

all_ps_tbl_blast_controls$`Final ID (BLASTn)` %>% unique() %>% sort()
          
all_ps_tbl_blast_controls_clean <- all_ps_tbl_blast_controls %>%
  # mutate("ID status" = case_when(is.na(`Final ID (BLASTn)`) & is.na(`Final ID (DADA2)` ) ~ "not IDed",
  # !is.na(`Final ID (BLASTn)`) | !is.na(`Final ID (DADA2)`) ~ "IDed")) %>%
  mutate("ID status" = case_when(is.na(`Final ID (BLASTn)`) ~ "not IDed",
  !is.na(`Final ID (BLASTn)`) ~ "IDed")) %>%
  group_by(
    Unique_File_name_Primer,
    # Sample,
    # `Sample name`,
    `Expected length`,`ID status`, `Possible Metazoa`,`Read origin`
    # ,`Possible contamination`
    ) %>%
    # group_by(Sample,`Expected length`,`ID status`, `Possible Metazoa`,`Read origin`,`Possible contamination`) %>%  #WWF
    mutate("Total clean sample abd."  = 0,
           "Total clean sample abd." = case_when((`ID status` %in% c("IDed") &
                                                    `Expected length` %in% c("in range")
                                                    # `Possible contamination` %in% c("True detection") &
                                                    # & `Possible Metazoa` == TRUE
                                                  ) ~ sum(`Abundance`),
                                                 (`ID status` %in% "not IDed") ~ 0,
                                                 (`Expected length` %in% "out of range") ~ 0
                                                 # (`Possible Metazoa` %in% FALSE) ~ 0
                                                 # (`Possible contamination` %in% "Possible contamination") ~ 0,
                                                 # TRUE ~ 0
                                                 )) %>%
  ungroup() %>% 
  # mutate("Clean relative abd. on sample" =  case_when(`ID status` == "not IDed"  ~ 0,
  #                                                     `Expected length` == "out of range" ~ 0,
  #                                                     `ID status` == "IDed" & 
  #                                                       `Expected length` == "in range" & 
  #                                                       `Possible contamination` == "True detection" &
  #                                                       `Possible Metazoa` == TRUE ~ (`Abundance`/`Total clean sample abd.`))) %>% 
  mutate("Clean relative abd. on sample" =  (`Abundance`/`Total clean sample abd.`*100)) %>% 
  relocate("Unique_File_name","Sample","Primer","Expected length","ID status", "Possible Metazoa","Read origin","Possible contamination","Clean relative abd. on sample","Total clean sample abd.")

#order by abundance
colnames(all_ps_tbl_blast_controls_clean)
unique(all_ps_tbl_blast_controls_clean$Remove)

all_ps_tbl_blast_controls_clean %>% 
  select(`Clean relative abd. on sample`,Sample,`Curated ID`) %>%  
  ggplot(aes(x = `Clean relative abd. on sample`, 
             y = Sample, 
             group = Sample,
             fill = `Curated ID`)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = viridis::turbo(n=1200),guide = "none")
```

## Save Final Tables

### Complete results table

```{r,echo=TRUE, eval=FALSE}

colnames(all_ps_tbl_blast_controls_clean) %>% paste0(collapse = '",\n"') %>% cat()

smp_abd_ID <- all_ps_tbl_blast_controls_clean %>% 
  filter(`Abundance` > 0) %>% 
  arrange(
    Unique_File_name,
    # Unique_File_name_Primer,
    `ID status`,
    desc(`Read origin`),
    desc(`Clean relative abd. on sample`),
    desc(`Relative abundance on sample`),
          ) %>%
  dplyr::rename(
    # "ASV (Sequence)" = "ASV (Sequence)",
    # "Sample" = "Sample",
    # "Preservation" = "Metadata 1",
    "ASV absolute abundance" = "Abundance",
    # "Sample Name" = "Sample.Name",
    # "Tag pairs" = "Tag.pairs",
    # "Exact Genus (DADA2)" = "Exact Genus (DADA2)",
    # "exact Species (DADA2)" = "exact Species (DADA2)",
    # "Exact Genus/Species (DADA2)" = "Exact GenSp (DADA2)",
    # "Kingdom (DADA2 boot)" = "(DADA2 bootstrap)Kingdom",
    # "Phylum (DADA2 boot)" = "(DADA2 bootstrap)Phylum",
    # "Class (DADA2 boot)" = "(DADA2 bootstrap)Class",
    # "Order (DADA2 boot)" = "(DADA2 bootstrap)Order",
    # "Family (DADA2 boot)" = "(DADA2 bootstrap)Family",
    # "Genus (DADA2 boot)" = "(DADA2 bootstrap)Genus",
    # "Species (DADA2 boot)" = "(DADA2 bootstrap)Species",
    # "Specimen (DADA2 boot)" = "(DADA2 bootstrap)Specimen",
    # "Basin (DADA2 boot)" = "(DADA2 bootstrap)Basin",
    # "Superkingdom (BLASTn)" = "Superkingdom (BLASTn)",
    # "BLAST ID" = "blast ID",
    # "Exact Genus and Species (DADA2)" = "exact GenSp",
    "Final ID (BLASTn)" = "Final ID (BLASTn)",
    # "Final ID (DADA2)" = "Final ID (DADA2)",
    "Contamination control" = "Remove",
    "Contamination status" = "Possible contamination",
    "Primer expected length" = "Expected length",
    ) %>%
  relocate(c(
    "Project",
    "BLASTn pseudo-score",
    "Identification",
    "Identification Max. taxonomy",
    "Primer",
    "Sample",
    "Unique_File_name",
    "Unique_File_name_Primer",
    "Read origin",
    "Relative abundance to all samples",
    "Relative abundance on sample",
    "Sample total abundance",
    "ASV absolute abundance",
    "Total clean sample abd.",
    "Clean relative abd. on sample",
    "Primer expected length",
    "ASV Size (pb)",
    "Possible Metazoa",
    "Curated ID",
    "Final ID (BLASTn)",
    "blast ID Origin",
    "ID status",
    "Contamination status",
    "Genus (BLASTn)",
    "Subfamily (BLASTn)",
    "Family (BLASTn)",
    "Suborder (BLASTn)",
    "Order (BLASTn)",
    "Subclass (BLASTn)",
    "Class (BLASTn)",
    "Phylum (BLASTn)",
    "Subphylum (BLASTn)",
    "Kingdom (BLASTn)",
    "Superkingdom (BLASTn)",
    "1_subject header",
    "1_staxid",
    "1_subject",
    "1_indentity",
    "1_qcovhsp",
    "1_length",
    "1_mismatches",
    "1_gaps",
    "1_query start",
    "1_query end",
    "1_subject start",
    "1_subject end",
    "1_e-value",
    "1_bitscore",
    "2_subject header",
    "2_staxid",
    "2_subject",
    "2_indentity",
    "2_qcovhsp",
    "2_length",
    "2_mismatches",
    "2_gaps",
    "2_query start",
    "2_query end",
    "2_subject start",
    "2_subject end",
    "2_e-value",
    "2_bitscore",
    "3_subject header",
    "3_staxid",
    "3_subject",
    "3_indentity",
    "3_qcovhsp",
    "3_length",
    "3_mismatches",
    "3_gaps",
    "3_query start",
    "3_query end",
    "3_subject start",
    "3_subject end",
    "3_e-value",
    "3_bitscore",
    "Contamination control",
    "ASV header",
    "ASV (Sequence)",
    "OTU",
    "PCR control",
    "Prop. to PCR control",
    "Type")) %>%
  select(-c(#"Curated ID",
            "Ext. control",
            "Filt. control"))

dim(smp_abd_ID)

colnames(smp_abd_ID) %>% paste0(collapse = '",\n"') %>% cat()

View(smp_abd_ID)

smp_abd_ID$Identification %>% unique()
smp_abd_ID$`Curated ID` %>% unique()

smp_abd_ID$`Contamination status` %>% unique()
smp_abd_ID$Sample %>% unique()
smp_abd_ID$`Superkingdom (BLASTn)` %>% unique()
smp_abd_ID$`Kingdom (BLASTn)` %>% unique()
smp_abd_ID$`Class (BLASTn)` %>% unique()

smp_abd_ID %>% group_by(Sample) %>%
# smp_abd_ID %>% group_by(Unique_File_name) %>%
  summarize("abd" = sum(`Clean relative abd. on sample`,na.rm = T),
            "tot" = sum(`Total clean sample abd.`,na.rm = T)/sum(!is.na(`Total clean sample abd.`))) %>% View()

# save complete table with all results ----

# selecionar apenas os resultados pertinentes ----

smp_abd_ID_Final <- smp_abd_ID 
# %>% 
  # filter(Primer %in% c("COIr1")) %>%
  # filter(Project %in% c("EM129_Florencia"))
  # filter(Project %in% c("EM135_CIAT")) 
  # filter(Researcher %in% c("EcoMol"))  
#           
# if (!dir.exists(paths = paste0(results_path,"/",unique(smp_abd_ID_Final$Project)))) {
#   dir.create(path = paste0(results_path,"/",unique(smp_abd_ID_Final$Project)))
#   print(paste0("Dir ",paste0(results_path,"/",unique(smp_abd_ID_Final$Project))," created"))
# }else{
#     print(paste0("Dir ",paste0(results_path,"/",unique(smp_abd_ID_Final$Project))," already exists"))
#   }
# 

# save complete results table, per project ----

for (project in all_projects) {

    project_name <- project %>% str_replace_all(pattern = " ",
                                        replacement = "_")
  
  smp_abd_ID_Final %>% 
    filter(project %in% c(project)) %>%
    writexl::write_xlsx(
      # path = paste0(results_path,"/",
      # unique(smp_abd_ID_Final$Project),"/",
      # unique(smp_abd_ID_Final$Project),"-todas_info_da_analise_",Sys.Date(),".xlsx"),
      path = paste0(results_path,"/",project_name,"-Complete_analysis_results-",Sys.Date(),".xlsx"),
      col_names = TRUE,
      format_headers = TRUE)
}
```

### Save ASV Vs. Samples table

```{r,echo=TRUE, eval=FALSE}

# generate ASVs Vs. Samples table from complete table ----

#function to either sum or unique by column type ----
sum_uniq <- function(vec=vec){
  
  if (is.character(vec)==TRUE) {
    suniq <- BiocGenerics::unique(vec)
  }
  if (is.numeric(vec)==TRUE) {
    suniq <- sum(vec)
  }
  return(suniq)
}

colnames(smp_abd_ID) %>% paste0(collapse = '",\n"') %>% cat()

smp_abd_ID$`ASV (Sequence)` %>% unique()

# generate ASVs Vs. Samples table from complete table ----

for (project in all_projects) {

project_name <- project %>% str_replace_all(pattern = " ",
                                        replacement = "_")

smp_abd_ID_eco <- smp_abd_ID_Final %>% 
  dplyr::mutate("Obs. Curadoria" = "") %>% 
  mutate("Superkingdom (BLASTn)" = "Eukaryota") %>%
  mutate(Identification = if_else(Identification %in% c(NA,"NA"),"NA",Identification)) %>%
  dplyr::select(-c("Relative abundance to all samples",
                   "Sample total abundance",
                   "ASV absolute abundance",
                   # "Metadata 1","Metadata 2","Metadata 3","Metadata 4","Metadata 5","obs",
                   # "Curated ID",
                   # "Identification",
                   # "Identification Max. taxonomy",
                   # "Final ID (BLASTn)",
                   # "Final ID (DADA2)",
                   # "Extraction.control",
                   "PCR control",
                   "Prop. to PCR control",
                   # "Prop. to Ext control",
                   # "Prop. to Filt control",
                   "Contamination status",
                   # "Primer expected length",
                   "Type")) %>%
  pivot_wider(
    id_cols = c(
      "Curated ID",
      "Obs. Curadoria",
      "Final ID (BLASTn)",
      # "Final ID (DADA2)",
      "Primer","Primer","Read origin","Primer expected length",
      "Possible Metazoa",
      "Identification",
      "Identification Max. taxonomy",
      # "Kingdom (DADA2)",
      # "Phylum (DADA2)",
      # "Class (DADA2)",
      # "Order (DADA2)",
      # "Family (DADA2)",
      # "Genus (DADA2)",
      # "Species (DADA2)",
      # "Specimen (DADA2)",
      # "Basin (DADA2)",
      # "Exact Genus/Species (DADA2)",
      "Superkingdom (BLASTn)",
      "Kingdom (BLASTn)",
      "Phylum (BLASTn)",
      "Subphylum (BLASTn)",
      "Class (BLASTn)",
      "Subclass (BLASTn)",
      "Order (BLASTn)",
      "Suborder (BLASTn)",
      "Family (BLASTn)",
      "Subfamily (BLASTn)",
      "Genus (BLASTn)",
      # "max_tax",
      # "BLAST ID",
      "BLASTn pseudo-score",
      "1_subject header","1_subject","1_indentity","1_qcovhsp",
      "1_length","1_mismatches","1_gaps","1_query start","1_query end",
      "1_subject start","1_subject end","1_e-value","1_bitscore","2_subject header",
      "2_subject","2_indentity","2_qcovhsp","2_length","2_mismatches",
      "2_gaps","2_query start","2_query end","2_subject start","2_subject end",
      "2_e-value","2_bitscore","3_subject header","3_subject","3_indentity",
      "3_qcovhsp","3_length","3_mismatches","3_gaps","3_query start",
      "3_query end","3_subject start","3_subject end","3_e-value","3_bitscore",
      # "Contamination control",
      "ASV Size (pb)","ASV header","ASV (Sequence)","OTU"),
    # values_from ="Relative abundance on sample",
    values_from ="Clean relative abd. on sample",
    values_fn = sum_uniq,
    names_from = "Unique_File_name_Primer",
    names_sort = TRUE,
    names_prefix = "SAMPLE ") %>%
  relocate(c("Primer",
             "Read origin",
             # "Kingdom (DADA2)",
             # "Phylum (DADA2)",
             # "Class (DADA2)",
             # "Order (DADA2)",
             # "Family (DADA2)",
             # "Genus (DADA2)",
             # "Species (DADA2)",
             # "Specimen (DADA2)",
             # "Basin (DADA2)",
             # "Exact Genus/Species (DADA2)",
             "Superkingdom (BLASTn)",
             "Kingdom (BLASTn)",
             "Phylum (BLASTn)",
             "Subphylum (BLASTn)",
             "Class (BLASTn)",
             "Subclass (BLASTn)",
             "Order (BLASTn)",
             "Suborder (BLASTn)",
             "Family (BLASTn)",
             "Subfamily (BLASTn)",
             "Genus (BLASTn)",
             "Possible Metazoa",
             "Final ID (BLASTn)",
             "Identification",
             "Identification Max. taxonomy",
             "BLASTn pseudo-score",
             "Curated ID",
             "Obs. Curadoria",
             "Primer expected length",
             "ASV Size (pb)",
             starts_with("SAMPLE "),
             "1_subject header",
             "1_subject",
             "1_indentity",
             "1_qcovhsp",
             "1_length",
             "1_mismatches",
             "1_gaps",
             "1_query start",
             "1_query end",
             "1_subject start",
             "1_subject end",
             "1_e-value",
             "1_bitscore",
             "2_subject header",
             "2_subject",
             "2_indentity",
             "2_qcovhsp",
             "2_length",
             "2_mismatches",
             "2_gaps",
             "2_query start",
             "2_query end",
             "2_subject start",
             "2_subject end",
             "2_e-value",
             "2_bitscore",
             "3_subject header",
             "3_subject",
             "3_indentity",
             "3_qcovhsp",
             "3_length",
             "3_mismatches",
             "3_gaps",
             "3_query start",
             "3_query end",
             "3_subject start",
             "3_subject end",
             "3_e-value",
             "3_bitscore",
             "ASV header",
             "ASV (Sequence)",
             "OTU"
             )) %>%
  mutate_if(is.numeric , replace_na, replace = 0)

smp_abd_ID_eco %>%
  writexl::write_xlsx(
    # path = paste0(results_path,"/",
    #               unique(smp_abd_ID_Final$Project),"/",
    #               unique(smp_abd_ID_Final$Project),
    #               "-todas_infos_ASVs-",Sys.Date(),".xlsx"),
    path = paste0(results_path,"/",project_name,"-ASVs_x_amostras-",Sys.Date(),".xlsx"),
    col_names = TRUE,format_headers = TRUE)
}

View(smp_abd_ID_eco)
dim(smp_abd_ID)
dim(smp_abd_ID_eco)
dim(smp_abd_ID_eco_ID)

colnames(smp_abd_ID_eco) %>% paste0(collapse = '",\n"') %>% cat()

```